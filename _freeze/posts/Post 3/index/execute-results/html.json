{
  "hash": "bb9e579cd8c88c92b427a52307b53ee4",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Final Project\"\nauthor: \"Emma Rasmussen\"\ndescription: \"Machine Learning to Interpret Cardiotocograms (CTGs)\"\ndate: \"05/22/2022\"\nimage: \"CTG_Output.jpeg\"\nformat:\n  html:\n    toc: true\n    code-copy: true\n    code-tools: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(VIM)\nlibrary(lubridate)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(e1071)\nlibrary(class)\nlibrary(nnet)\nlibrary(boot)\nlibrary(caret)\nlibrary(MLmetrics)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(fastDummies)\n\nvirtualenv_create(\"MLFinal\") #create virtual environment... per reticulate cheat sheet\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvirtualenv: MLFinal\n```\n:::\n\n```{.r .cell-code}\npy_install(\"pandas\", env_name=\"MLFinal\")\npy_install(\"numpy\", env_name=\"MLFinal\")\npy_install(\"xgboost\", env_name=\"MLFinal\")\nconda_install(\"MLFinal\", \"scikit-learn\")\n\nuse_virtualenv(\"MLFinal\")\n\nknitr::knit_engines$set(python =\nreticulate::eng_python) \n```\n:::\n\n\n# Motivation\n\nCardiotocograms (CTGs) are used during labor and late stage pregnancy to monitor fetal and maternal health. CTGs (also called electronic fetal monitoring, EFM) use ultrasound transducers to record fetal heart rate and Braxton Hicks contractions. CTGs can indicate if a fetus is at risk for hypoxia (lack of oxygen). Variables like histogram_mode (etc) are referring to fetal heart rate histograms. An example CTG is pictured below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](CTG_Output.jpeg){width=438}\n:::\n:::\n\n\nImage Source: (Cardiotocography 2023)\n\nOn the CTG, (A) shows fetal heartbeat, (B) shows fetal movement felt by the mother, (C) shows recorded fetal movement, and (D) shows uterine contractions.\n\nA fetus at risk for hypoxia is considered \"pathological\" and in need of immediate attention. Suspect cases may have one characteristic that strays from normal but are not at an immediate risk.\n\nIn the fetal health data set used in this analysis, the classification into categories (normal (1), suspect (2), and pathological (3)) was done by \"expert obstetricians\" interpreting the CTG results (Ayres de Campos et al. 2000).\n\nThe motivation behind creating a machine learning algorithm to interpret these CTGs is to:\n\n-   Aid human interpretation of CTGs, make results more accessible to untrained individuals\n\n-   Speed up human interpretation\n\n-   Potentially help reduce biases in the medical system (for instance, discounting an individuals concerns based on race or ethnicity). An algorithm based on the variables in our dataset should not hold these biases.\n\nThis document is created with R Markdown and uses R's 'reticulate' package to integrate Python code. Most visualizations and data cleaning is done in R, while the machine learning algorithms are run in Python.\n\n# Exploratory Data Analysis and Data Preprocessing\n\nRead in the Data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfet_df<- read.csv(\"fetal_health.csv\")\nfet_df$fetal_health<-as.factor(fet_df$fetal_health)\n```\n:::\n\n\nLook at size of data frame and summary statistics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(fet_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2126   22\n```\n:::\n\n```{.r .cell-code}\nsummary(fet_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n baseline.value  accelerations      fetal_movement     uterine_contractions\n Min.   :106.0   Min.   :0.000000   Min.   :0.000000   Min.   :0.000000    \n 1st Qu.:126.0   1st Qu.:0.000000   1st Qu.:0.000000   1st Qu.:0.002000    \n Median :133.0   Median :0.002000   Median :0.000000   Median :0.004000    \n Mean   :133.3   Mean   :0.003178   Mean   :0.009481   Mean   :0.004366    \n 3rd Qu.:140.0   3rd Qu.:0.006000   3rd Qu.:0.003000   3rd Qu.:0.007000    \n Max.   :160.0   Max.   :0.019000   Max.   :0.481000   Max.   :0.015000    \n light_decelerations severe_decelerations prolongued_decelerations\n Min.   :0.000000    Min.   :0.000e+00    Min.   :0.0000000       \n 1st Qu.:0.000000    1st Qu.:0.000e+00    1st Qu.:0.0000000       \n Median :0.000000    Median :0.000e+00    Median :0.0000000       \n Mean   :0.001889    Mean   :3.293e-06    Mean   :0.0001585       \n 3rd Qu.:0.003000    3rd Qu.:0.000e+00    3rd Qu.:0.0000000       \n Max.   :0.015000    Max.   :1.000e-03    Max.   :0.0050000       \n abnormal_short_term_variability mean_value_of_short_term_variability\n Min.   :12.00                   Min.   :0.200                       \n 1st Qu.:32.00                   1st Qu.:0.700                       \n Median :49.00                   Median :1.200                       \n Mean   :46.99                   Mean   :1.333                       \n 3rd Qu.:61.00                   3rd Qu.:1.700                       \n Max.   :87.00                   Max.   :7.000                       \n percentage_of_time_with_abnormal_long_term_variability\n Min.   : 0.000                                        \n 1st Qu.: 0.000                                        \n Median : 0.000                                        \n Mean   : 9.847                                        \n 3rd Qu.:11.000                                        \n Max.   :91.000                                        \n mean_value_of_long_term_variability histogram_width  histogram_min   \n Min.   : 0.000                      Min.   :  3.00   Min.   : 50.00  \n 1st Qu.: 4.600                      1st Qu.: 37.00   1st Qu.: 67.00  \n Median : 7.400                      Median : 67.50   Median : 93.00  \n Mean   : 8.188                      Mean   : 70.45   Mean   : 93.58  \n 3rd Qu.:10.800                      3rd Qu.:100.00   3rd Qu.:120.00  \n Max.   :50.700                      Max.   :180.00   Max.   :159.00  \n histogram_max histogram_number_of_peaks histogram_number_of_zeroes\n Min.   :122   Min.   : 0.000            Min.   : 0.0000           \n 1st Qu.:152   1st Qu.: 2.000            1st Qu.: 0.0000           \n Median :162   Median : 3.000            Median : 0.0000           \n Mean   :164   Mean   : 4.068            Mean   : 0.3236           \n 3rd Qu.:174   3rd Qu.: 6.000            3rd Qu.: 0.0000           \n Max.   :238   Max.   :18.000            Max.   :10.0000           \n histogram_mode  histogram_mean  histogram_median histogram_variance\n Min.   : 60.0   Min.   : 73.0   Min.   : 77.0    Min.   :  0.00    \n 1st Qu.:129.0   1st Qu.:125.0   1st Qu.:129.0    1st Qu.:  2.00    \n Median :139.0   Median :136.0   Median :139.0    Median :  7.00    \n Mean   :137.5   Mean   :134.6   Mean   :138.1    Mean   : 18.81    \n 3rd Qu.:148.0   3rd Qu.:145.0   3rd Qu.:148.0    3rd Qu.: 24.00    \n Max.   :187.0   Max.   :182.0   Max.   :186.0    Max.   :269.00    \n histogram_tendency fetal_health\n Min.   :-1.0000    1:1655      \n 1st Qu.: 0.0000    2: 295      \n Median : 0.0000    3: 176      \n Mean   : 0.3203                \n 3rd Qu.: 1.0000                \n Max.   : 1.0000                \n```\n:::\n:::\n\n\nThe data has 2126 observations and 22 variables.\n\nCheck for missing values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolSums(is.na(fet_df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                        baseline.value \n                                                     0 \n                                         accelerations \n                                                     0 \n                                        fetal_movement \n                                                     0 \n                                  uterine_contractions \n                                                     0 \n                                   light_decelerations \n                                                     0 \n                                  severe_decelerations \n                                                     0 \n                              prolongued_decelerations \n                                                     0 \n                       abnormal_short_term_variability \n                                                     0 \n                  mean_value_of_short_term_variability \n                                                     0 \npercentage_of_time_with_abnormal_long_term_variability \n                                                     0 \n                   mean_value_of_long_term_variability \n                                                     0 \n                                       histogram_width \n                                                     0 \n                                         histogram_min \n                                                     0 \n                                         histogram_max \n                                                     0 \n                             histogram_number_of_peaks \n                                                     0 \n                            histogram_number_of_zeroes \n                                                     0 \n                                        histogram_mode \n                                                     0 \n                                        histogram_mean \n                                                     0 \n                                      histogram_median \n                                                     0 \n                                    histogram_variance \n                                                     0 \n                                    histogram_tendency \n                                                     0 \n                                          fetal_health \n                                                     0 \n```\n:::\n:::\n\n\nThere are no missing values in the dataframe so I will not be doing data imputation for NAs.\n\nLook at dependent variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=fet_df, aes(x= fetal_health))+geom_bar(stat=\"count\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Number of individuals in each category \ntable(fet_df$fetal_health)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   1    2    3 \n1655  295  176 \n```\n:::\n\n```{.r .cell-code}\n#percent of observations belonging to each category\n(table(fet_df$fetal_health))/2126\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n         1          2          3 \n0.77845720 0.13875823 0.08278457 \n```\n:::\n:::\n\n\nThere are 1655 normal cases ('1', 77.8%), 295 suspect cases ('2', 13.9%), and 176 pathological cases ('3', 8.3%), indicating an unbalanced outcome variable.\n\nLooking at correlations between independent variables and outcome class:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng1<-ggplot(data=fet_df, aes(x= baseline.value))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng2<-ggplot(data=fet_df, aes(x= accelerations))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng3<-ggplot(data=fet_df, aes(x= fetal_movement))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng4<-ggplot(data=fet_df, aes(x= uterine_contractions))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng5<-ggplot(data=fet_df, aes(x= light_decelerations))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng6<-ggplot(data=fet_df, aes(x= severe_decelerations))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng7<-ggplot(data=fet_df, aes(x= prolongued_decelerations))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng8<-ggplot(data=fet_df, aes(x= abnormal_short_term_variability))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng9<-ggplot(data=fet_df, aes(x= mean_value_of_short_term_variability))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng10<-ggplot(data=fet_df, aes(x= percentage_of_time_with_abnormal_long_term_variability))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng11<-ggplot(data=fet_df, aes(x= histogram_width))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng12<-ggplot(data=fet_df, aes(x= mean_value_of_long_term_variability))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng13<-ggplot(data=fet_df, aes(x= histogram_min))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng14<-ggplot(data=fet_df, aes(x= histogram_max))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng15<-ggplot(data=fet_df, aes(x= histogram_number_of_peaks))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng16<-ggplot(data=fet_df, aes(x= histogram_number_of_zeroes))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng17<-ggplot(data=fet_df, aes(x= histogram_mode))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng18<-ggplot(data=fet_df, aes(x= histogram_mean))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng19<-ggplot(data=fet_df, aes(x= histogram_median))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng20<-ggplot(data=fet_df, aes(x= histogram_variance))+geom_bar(stat=\"count\", aes(fill=fetal_health))\ng21<-ggplot(data=fet_df, aes(x= histogram_tendency))+geom_bar(stat=\"count\", aes(fill=fetal_health))\n\ngrid.arrange(g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngrid.arrange(g12, g13, g14, g15, g16, g17, g18, g19, g20, g21)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\nLooking for outliers in independent variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng1<-ggplot(data=fet_df, aes(x=fetal_health, y=baseline.value))+geom_boxplot()\ng2<-ggplot(data=fet_df, aes(x=fetal_health, y=accelerations))+geom_boxplot()\ng3<-ggplot(data=fet_df, aes(x=fetal_health, y=fetal_movement))+geom_boxplot()\ng4<-ggplot(data=fet_df, aes(x=fetal_health, y=uterine_contractions))+geom_boxplot()\ng5<-ggplot(data=fet_df, aes(x=fetal_health, y=light_decelerations))+geom_boxplot()\ng6<-ggplot(data=fet_df, aes(x=fetal_health, y=severe_decelerations))+geom_boxplot()\ng7<-ggplot(data=fet_df, aes(x=fetal_health, y=prolongued_decelerations))+geom_boxplot()\ng8<-ggplot(data=fet_df, aes(x=fetal_health, y=abnormal_short_term_variability))+geom_boxplot()\ng9<-ggplot(data=fet_df, aes(x=fetal_health, y=mean_value_of_short_term_variability))+geom_boxplot()\ng10<-ggplot(data=fet_df, aes(x=fetal_health, y=percentage_of_time_with_abnormal_long_term_variability))+geom_boxplot()\ng11<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_width))+geom_boxplot()\ng12<-ggplot(data=fet_df, aes(x=fetal_health, y=mean_value_of_long_term_variability))+geom_boxplot()\ng13<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_min))+geom_boxplot()\ng14<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_max))+geom_boxplot()\ng15<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_number_of_peaks))+geom_boxplot()\ng16<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_number_of_zeroes))+geom_boxplot()\ng17<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_mode))+geom_boxplot()\ng18<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_mean))+geom_boxplot()\ng19<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_median))+geom_boxplot()\ng20<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_variance))+geom_boxplot()\ng21<-ggplot(data=fet_df, aes(x=fetal_health, y=histogram_tendency))+geom_boxplot()\n\ngrid.arrange(g1, g2, g3, g4, g5, g6, g7)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngrid.arrange(g8, g9, g10, g11, g12, g13, g14)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\ngrid.arrange(g15, g16, g17, g18, g19, g20, g21)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n:::\n\n\nThere may be outliers based on some of the variables. I will run models on sets with all observations, but also try running it on data where the mean is imputed for the outliers to see if this makes a difference in model performance.\n\n## Creating Dummy Variables from Histogram Tendency Variable\n\nSince analysis will be done in python, I need categorical variables coded as dummy variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfet_df<-mutate(fet_df, histogram_tendency=(case_when(\n                                   histogram_tendency == -1 ~ \"negative\",\n                                   histogram_tendency == 0 ~ \"zero\",\n                                   histogram_tendency == 1 ~ \"positive\")))\n\n#using fastDummies package\nfet_df<- dummy_cols(fet_df, select_columns = 'histogram_tendency')\nhead(dplyr::select(fet_df, 'histogram_tendency_negative', 'histogram_tendency_positive', 'histogram_tendency_zero'), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  histogram_tendency_negative histogram_tendency_positive\n1                           0                           1\n2                           0                           0\n3                           0                           0\n4                           0                           1\n5                           0                           1\n  histogram_tendency_zero\n1                       0\n2                       1\n3                       1\n4                       0\n5                       0\n```\n:::\n:::\n\n\n## Creating a Binary Outcome Variables\n\nAs I ran the models, I realized all models have a difficult time detecting suspect cases in particular. I combined suspect and pathological classes into a binary variable to see if this might improve performance when it comes to detecting these categories.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#mutating outcome to be binary 1= normal, 2= suspect & pathological. New outcome column is called fetal_health_bin\n\n#case_when does not work well with factors, so I converted to characters and then back to factors\nfet_df$fetal_health<-as.character(fet_df$fetal_health)\n\n#Mutate into new binary outcome column fetal_health_bin (0=normal, 1= flagged cases)\nfet_df<-fet_df %>% \n  mutate(fetal_health_bin=case_when(\n  fetal_health== \"1\" ~ \"0\",\n  fetal_health == \"2\" | fetal_health == \"3\" ~ \"1\"))\n\n#fix classes of mutated columns\nfet_df$fetal_health_bin<-as.factor(fet_df$fetal_health_bin)\nfet_df$fetal_health<-as.factor(fet_df$fetal_health)\n```\n:::\n\n\n## Creating Ordered Factors for 3 Class Outcome:\n\nI also tested an ordered factor outcome category throughout runs, but to save space and time running the code I removed this from the analysis. Ordering the outcome factors generally resulted in the same outcome as for unordered classes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#trying out ordered factors\nfet_df$fetal_health_fac <- factor(fet_df$fetal_health, ordered = TRUE, \n                                levels = c(\"1\", \"2\", \"3\"))\n\n\n#Checking classes of new columns\nclass(fet_df$fetal_health_fac)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ordered\" \"factor\" \n```\n:::\n\n```{.r .cell-code}\nclass(fet_df$fetal_health)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"factor\"\n```\n:::\n:::\n\n\n## Splitting the Data\n\nI am setting aside the test (hold-out) set for use after cross-validation/ hyperparameter optimization. The training set has 1488 observations (70% of the data) and the test set has 638 observations (30% of the data).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create ID column\nfet_df$id<- 1:nrow(fet_df)\n\nset.seed(12)\n\n#use 70% of dataset as training set and 30% as test set \ntrain <- fet_df %>% sample_frac(0.7) \ntest <- anti_join(fet_df, train, by = 'id')\n\n#Checking dimensions of split data\ndim(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1488   28\n```\n:::\n\n```{.r .cell-code}\ndim(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 638  28\n```\n:::\n:::\n\n\nThe rest of the data preprocessing (scaling and outlier imputing) will be done within each cross-validation split using sci-kit learn's pipeline.\n\n# Evaluation Metric\n\nA metric that places importance on the detection of categories \"suspect\" (2) and \"pathological\" (3) is going to be very important so more caution and attention will be paid to these higher-risk cases. In other words, false negatives are likely to be more harmful than false positives. For instance, we can get very good accuracy score if we are very successful in classifying normal cases but less successful in detecting pathological and suspect cases.\n\nIn addition to macro f1 score, I will consider by-class f1 scores to ensure the model is performing sufficiently on suspect and pathological cases.\n\n# Fit Models\n\n### Python Set-Up\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np \nimport pandas as pd \nimport sklearn as sklearn\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n#Opening R objects with reticulate/Python\ntrain=r.train\ntest=r.test\n  \n#Checking class of outcome variable\ntrain.dtypes['fetal_health']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCategoricalDtype(categories=['1', '2', '3'], ordered=False)\n```\n:::\n\n```{.python .cell-code}\ntrain.dtypes['fetal_health_fac']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCategoricalDtype(categories=['1', '2', '3'], ordered=True)\n```\n:::\n\n```{.python .cell-code}\ntrain.dtypes['fetal_health_bin']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCategoricalDtype(categories=['0', '1'], ordered=False)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n#Setting subsets for x and y variables\n\ntrain_x=train[['baseline.value',\n'accelerations',\n'fetal_movement',\n'uterine_contractions',\n'light_decelerations',\n'severe_decelerations',\n'prolongued_decelerations',\n'abnormal_short_term_variability',\n'mean_value_of_short_term_variability',\n'percentage_of_time_with_abnormal_long_term_variability',\n'mean_value_of_long_term_variability',\n'histogram_width',\n'histogram_min',\n'histogram_max',\n'histogram_number_of_peaks',\n'histogram_number_of_zeroes',\n'histogram_mode',\n'histogram_mean',\n'histogram_median',\n'histogram_variance',\n'histogram_tendency_negative',\n'histogram_tendency_zero',\n'histogram_tendency_positive']]\n\n#Outcome Class for 3 Classes\ntrain_y=train[['fetal_health']]\n\n#Outcome Class for Ordered 3 Classes\ntrain_y_fac=train[['fetal_health_fac']]\n\n#Outcome Class for Binary\ntrain_y_bin=train[['fetal_health_bin']]\n\n#convert data type from matrix to numpy ndarray\ntrain_y=train_y.values.ravel()\ntrain_y_fac=train_y_fac.values.ravel()\ntrain_y_bin=train_y_bin.values.ravel()\n```\n:::\n\n\n### Defining a Function to Remove Outliers Based on Z-score\n\nI used ChatGPT to help with this code. I still need to learn more about working with Numpy arrays and Pandas data frames but this eventually helped me figure out a function that could be implemented with Scikit-learn's pipeline.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#changing class of data frame to work with Function Transformer\nnp.array(train) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[135.0, 0.001, 0.002, ..., '0', '1', 450],\n       [139.0, 0.0, 0.007, ..., '1', '2', 346],\n       [146.0, 0.0, 0.003, ..., '1', '3', 336],\n       ...,\n       [115.0, 0.006, 0.0, ..., '0', '1', 1275],\n       [138.0, 0.0, 0.0, ..., '0', '1', 843],\n       [122.0, 0.005, 0.0, ..., '0', '1', 931]], dtype=object)\n```\n:::\n\n```{.python .cell-code}\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n#This code (from ChatGPT) transforms a function to work within Pipeline\nclass ElementWiseFunctionTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, func):\n        self.func = func\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return self.func(X)\n      \n#Defining a function that replaces standardized observations with a z-score greater than the absolute value of 3 with 0 (mean). Observations will be standardized in the previous step in the pipe\ndef replace_greater_than_abs_three(arr):\n    return np.where(np.abs(arr) > 3, 0, arr)\n```\n:::\n\n\n## Penalized Logistic Regression\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#import python modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nimport random\n```\n:::\n\n\n### Testing l1 and l2 Penalty\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#setting parameters to be tested with GridSearchCV\nparams = [\n{'log_l__C': [0.01,0.1,1,10,100,500,1000],\n'log_l__penalty': ['l1', 'l2']} # testing l1 and l2 penalty\n]\n\n#define pipeline\nlog_pipe = Pipeline(steps=[('scale', StandardScaler()), ('log_l', LogisticRegression(solver='saga', tol=0.006))])\n\n#define pipeline that also removes outliers\nlog_pipe_no_out = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('log_l', LogisticRegression(solver='saga', tol=0.006))])\n\n#Apply Grid Search\nGS_log = GridSearchCV(log_pipe, param_grid=params, scoring=\"f1_macro\", cv=5) #with outliers\nGS_log_no_out = GridSearchCV(log_pipe_no_out, param_grid=params, scoring=\"f1_macro\", cv=5) #with outliers imputed\n```\n:::\n\n\nUsing the pipeline should apply transformations at each fold to avoid data leakage and get more accurate evaluation metrics.\n\n### Defining a Function to Return Macro f1 Scores and By-Class F1 Scores\n\nBelow I define a function that takes in the outcome variable (y) and pre-defined GridSearchCV object and outputs macro f1 and by-class f1 scores\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef CV_F1_function(GS_object, outcome):\n  \n  random.seed(3) # set seed each time the algorithm is run for reproducibility\n  \n  #print best parameters and macro f1\n  best_GS=GS_object.fit(train_x, outcome) #finds the best parameters\n  print(best_GS.best_params_)\n  print(f'Best Macro F1:  {best_GS.best_score_}')\n  \n  #Print by-class f1 scores \n  pred1 = cross_val_predict(best_GS.best_estimator_, train_x, outcome, cv=5) #plugs in best parameters\n  f1_class = f1_score(outcome, pred1, average=None) #calculates by-class f1 scores\n  print(f1_class)\n```\n:::\n\n\nI use the above function to determine best parameters and optimize macro f1 scores for our data with outliers and with the outliers removed and imputed with the mean (0). This function returns optimized parameters, macro f1 scores, and by-class f1 scores (in the order: normal, suspect, pathological). I use this function throughout the rest of this document.\n\nPlugging logistic regression grid search objects into function:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nCV_F1_function(GS_object=GS_log, outcome=train_y) #3 Classes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_l__C': 1, 'log_l__penalty': 'l1'}\nBest Macro F1:  0.8194031154528476\n[0.9524618  0.70952381 0.8       ]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_log_no_out, outcome=train_y) #3 Classes with x variable outliers removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_l__C': 500, 'log_l__penalty': 'l2'}\nBest Macro F1:  0.7851217297163775\n[0.94282084 0.67990074 0.72641509]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_log, outcome=train_y_bin) #Binary Outcome\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_l__C': 1000, 'log_l__penalty': 'l1'}\nBest Macro F1:  0.8584997817538043\n[0.9407282  0.77198697]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_log_no_out, outcome=train_y_bin) #Binary Outcome with x variable outliers removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_l__C': 10, 'log_l__penalty': 'l1'}\nBest Macro F1:  0.8374660860773042\n[0.93316413 0.74183007]\n```\n:::\n:::\n\n\nOur best macro f1 score for the 3 class unordered outcome (outliers kept) is 0.8194 where the f1 for \"normal\" is 0.952, 0.7095 for \"suspect\", and 0.8 for \"pathological\". The optimized parameters are an l1 penalty and C=1 (C is 1/lambda, so a small C indicates a large penalty).\n\nFor the binary outcome, out best f1 score is 0.8585 (occurs when outliers are kept) with an f1 for normal of 0.9416 and for the suspect/pathological cases 0.7752. The optimized C is 100, (a smaller penalty than for the 3 class outcome) and once again, the l1 penalty.\n\n## Elastic Net Regularization\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nparams = [\n{'log_net__l1_ratio': [0.2, 0.5, 0.7, 0.8, 0.9, 1], #1 indicates full f1 penalty \n'log_net__C': [0.01, 0.1, 1.0, 10, 100]} # tests 20 values of C between 0 and 4 on the log scale}\n]\n\nlog_net = LogisticRegression(solver='saga', tol=0.006, penalty='elasticnet')\n\nlog_net_pipe = Pipeline(steps=[('scale', StandardScaler()), ('log_net', LogisticRegression(solver='saga', tol=0.006, penalty='elasticnet'))])\n\nlog_net_pipe_no_out = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('log_net', LogisticRegression(solver='saga', tol=0.006, penalty='elasticnet'))])\n\nGS_log_net = GridSearchCV(log_net_pipe, param_grid=params, scoring=\"f1_macro\", cv=5)\nGS_log_net_no_out = GridSearchCV(log_net_pipe_no_out, param_grid=params, scoring=\"f1_macro\", cv=5)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\nCV_F1_function(GS_object=GS_log_net, outcome=train_y) #3 Classes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_net__C': 1.0, 'log_net__l1_ratio': 0.9}\nBest Macro F1:  0.8194031154528476\n[0.9520577  0.70644391 0.8       ]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_log_net_no_out, outcome=train_y) #Outliers removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_net__C': 1.0, 'log_net__l1_ratio': 0.7}\nBest Macro F1:  0.7854264924167683\n[0.94296578 0.68304668 0.73267327]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_log_net, outcome=train_y_bin) #Binary Outcome\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_net__C': 100, 'log_net__l1_ratio': 0.2}\nBest Macro F1:  0.8584997817538043\n[0.94157494 0.7752443 ]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_log_net_no_out, outcome=train_y_bin)#Outliers Removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'log_net__C': 10, 'log_net__l1_ratio': 0.2}\nBest Macro F1:  0.8374660860773042\n[0.93316413 0.74183007]\n```\n:::\n:::\n\n\nAgain, our best macro f1 score for the unordered 3 class outcome (outliers kept) is 0.8194. The optimized C is still 1, and the l1 ratio is 0.9, indicating a stronger l1 penalty performs better. This is consistent with the previous runs finding the l1 penalty outperformed the l2 penalty. For the binary outcome, the macro f1 score is 0.8589. This is very close to the results from the previous section. For simplicity's sake I will only test the optimized parameters with the l1 penalty from the previous section on the final test set.\n\n## Boosting\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport random\nrandom.seed(1)\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n#While it is unnecessary to scale data for boosting, I include it so we can use it to remove outliers in the next pipeline. Scaling the data should not affect model performance.\nxgb_pipe = Pipeline(steps=[('scale', StandardScaler()), ('xgb', xgb.XGBClassifier(objective='multi:softprob'))]) #multi::softporb is used for multiclass outcome variables\n\nxgb_pipe_no_out = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('xgb', xgb.XGBClassifier(objective= 'multi:softprob'))]) \n\nxgb_pipe_bin = Pipeline(steps=[('scale', StandardScaler()), ('xgb', xgb.XGBClassifier(objective='binary:logistic'))]) #binary:logisitic is used for binary outcome variables\n\nxgb_pipe_no_out_bin = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('xgb', xgb.XGBClassifier(objective= 'binary:logistic'))]) \n\n#setting parameters to grid search\nparams = {\n  \"xgb__n_estimators\": [50, 100, 300, 500, 1000], #while more trees could increase performance slightly for some models, this is too time consuming to run\n  \"xgb__learning_rate\": [0.01, 0.1, 0.5, 1],\n  \"xgb__max_depth\": [1,4,6,8] #max depth of a tree... I tried to tune to higher depths however this was very time consuming to run and didn't provide much in return/ could possibly lead to overfitting\n}#setting parameters to grid search\n\n#fixing class outcome labels (0,1,2, work with xgboost as opposed to 1,2,3)\nle = LabelEncoder()\ntrain_yb = le.fit_transform(train_y)\ntrain_yb_fac = le.fit_transform(train_y_fac)\n\n#fixing class of binary outcome to work with xgboost\ntrain_yb_bin = le.fit_transform(train_y_bin)\n\n#Implementing grid search with cross validation\nGS_xgb= GridSearchCV(xgb_pipe, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\nGS_xgb_no_out= GridSearchCV(xgb_pipe_no_out, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\n\nGS_xgb_bin= GridSearchCV(xgb_pipe_bin, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\nGS_xgb_no_out_bin= GridSearchCV(xgb_pipe_no_out_bin, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\nCV_F1_function(GS_object=GS_xgb, outcome=train_yb) #3 Classes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'xgb__learning_rate': 0.5, 'xgb__max_depth': 8, 'xgb__n_estimators': 50}\nBest Macro F1:  0.908639338113707\n[0.97113752 0.84367246 0.9124424 ]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_xgb_no_out, outcome=train_yb) #Outliers Removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'xgb__learning_rate': 1, 'xgb__max_depth': 8, 'xgb__n_estimators': 300}\nBest Macro F1:  0.8891457194425483\n[0.97164621 0.81518987 0.88073394]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_xgb_bin, outcome=train_yb_bin) #Binary Outcome\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'xgb__learning_rate': 0.1, 'xgb__max_depth': 8, 'xgb__n_estimators': 300}\nBest Macro F1:  0.9328886201134079\n[0.97193878 0.89423077]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_xgb_no_out_bin, outcome=train_yb_bin) #Outliers Removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'xgb__learning_rate': 0.1, 'xgb__max_depth': 6, 'xgb__n_estimators': 1000}\nBest Macro F1:  0.9353604956079307\n[0.97274276 0.89808917]\n```\n:::\n:::\n\n\nThese are the best performing models so far. The model with the binary outcome class (with outliers removed) performs fairly well on the suspect/pathological outcome class (macro f1= 0.935, f1 for \"normal\"=0.972, and f1 for \"suspect/pathological\"=0.898).\n\n## Support Vector Machines (SVM)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn import svm\n\nparams= {\n  'svm__C': [1, 10, 100, 200], #misclassification error term\n  'svm__gamma': [1, 0.1, 0.01, 0.001], #distance of points to decision boundary being considered\n  'svm__kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n}\n\nsvm_cl_pipe = Pipeline([('scale', StandardScaler()), ('svm', svm.SVC())])\n\nsvm_cl_pipe_no_out = Pipeline([('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('svm', svm.SVC())])\n\nGS_svm= GridSearchCV(svm_cl_pipe, param_grid=params, scoring = \"f1_macro\", cv=5)\nGS_svm_no_out= GridSearchCV(svm_cl_pipe_no_out, param_grid=params, scoring = \"f1_macro\", cv=5)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\nCV_F1_function(GS_object=GS_svm, outcome=train_y) #Unordered 3 Classes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.8468817212083832\n[0.95939086 0.73658537 0.85148515]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_svm_no_out, outcome=train_y) #Outliers Removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.8376867178470627\n[0.95379398 0.71921182 0.8436019 ]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_svm, outcome=train_y_bin) #Binary Outcome\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.904418810931511\n[0.96006797 0.8488746 ]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_svm_no_out, outcome=train_y_bin) #Outliers Removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.8993948141236142\n[0.95737425 0.84126984]\n```\n:::\n:::\n\n\nAgain the 3 class model has trouble detecting suspect cases, so in this case, I might prefer the binary outcome model (where suspect and pathological classes combined f1 score is 0.8489.) Boosting still performs better for both the three class and binary outcome variables.\n\n## K-Nearest Neighbors (KNN)\n\nFinally I tested K nearest neighbors to see if a high variance model (low bias) may perform better than boosting. This is not the case.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nparams = {'knn__n_neighbors': [1, 2, 3, 5, 10]}\n\nknn_pipe = Pipeline([('scale', StandardScaler()), ('knn', KNeighborsClassifier())])\n\nknn_pipe_no_out = Pipeline([('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('knn', KNeighborsClassifier())])\n\nGS_knn= GridSearchCV(knn_pipe, param_grid=params, scoring = \"f1_macro\", cv=5)\nGS_knn_no_out= GridSearchCV(knn_pipe_no_out, param_grid=params, scoring = \"f1_macro\", cv=5)\n\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\nCV_F1_function(GS_object=GS_knn, outcome=train_y) #3 Classes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'knn__n_neighbors': 1}\nBest Macro F1:  0.8164438106575634\n[0.94661017 0.65835411 0.84651163]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_knn_no_out, outcome=train_y) #Outliers Removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'knn__n_neighbors': 3}\nBest Macro F1:  0.7970359617201562\n[0.94745621 0.66666667 0.77832512]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_knn, outcome=train_y_bin) #Binary Outcome\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'knn__n_neighbors': 3}\nBest Macro F1:  0.8756797607140687\n[0.9510665 0.8      ]\n```\n:::\n\n```{.python .cell-code}\nCV_F1_function(GS_object=GS_knn_no_out, outcome=train_y_bin) #Outliers Removed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'knn__n_neighbors': 3}\nBest Macro F1:  0.8781885851854287\n[0.95130143 0.8047138 ]\n```\n:::\n:::\n\n\n## Final Model Evaluation\n\n### Test set pre-processing\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Test set pre-processing\n#Setting subsets for x and y variables\n\ntest_x=test[['baseline.value',\n'accelerations',\n'fetal_movement',\n'uterine_contractions',\n'light_decelerations',\n'severe_decelerations',\n'prolongued_decelerations',\n'abnormal_short_term_variability',\n'mean_value_of_short_term_variability',\n'percentage_of_time_with_abnormal_long_term_variability',\n'mean_value_of_long_term_variability',\n'histogram_width',\n'histogram_min',\n'histogram_max',\n'histogram_number_of_peaks',\n'histogram_number_of_zeroes',\n'histogram_mode',\n'histogram_mean',\n'histogram_median',\n'histogram_variance',\n'histogram_tendency_negative',\n'histogram_tendency_zero',\n'histogram_tendency_positive']]\n\n#Outcome Class for 3 Classes\ntest_y=test[['fetal_health']]\n\n#Outcome Class for Binary\ntest_y_bin=test[['fetal_health_bin']]\n\n#convert data type from matrix to numpy npdarray\ntest_y=test_y.values.ravel()\ntest_y_bin=test_y_bin.values.ravel()\n\n#Standardize Test Data Based on column means and standard deviations of training set\ntest_x = (test_x - train_x.mean()) / train_x.std()\ntest_x_no_out=replace_greater_than_abs_three(test_x) #create test df with outliers imputed with the mean (0)\n\n#For simplicity, all variables (including dummy variables) have been standardized (the pipeline did this as well). While unnecessary, this should not affect results. \n\n#Standardize full training data\ntrain_x_st = (train_x - train_x.mean()) / train_x.std()\n\ntrain_x_st_no_out=replace_greater_than_abs_three(train_x_st) #create training df with outliers imputed with the mean\n```\n:::\n\n\n### Logistic Regression\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom.seed(3)\n#3 Classes, Outliers Kept: l1 penalty with C=1\nlog_final_mod=LogisticRegression(solver='saga', tol=0.006, C=1.0, penalty='l1') #fit model with optimized hyperparameters\nlog_final_mod.fit(train_x_st, train_y)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;, tol=0.006)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;, tol=0.006)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=log_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7603825352102777\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_y, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome, Outliers Kept: l1 penalty and C=500\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.9402229  0.58682635 0.75409836]\n```\n:::\n\n```{.python .cell-code}\nlog_final_mod_bin=LogisticRegression(solver='saga', tol=0.006, C=500, penalty='l1') #fit model with optimized hyperparameters\nlog_final_mod.fit(train_x_st, train_y_bin)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;, tol=0.006)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;, tol=0.006)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=log_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8712133867914023\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_y_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.94105691 0.80136986]\n```\n:::\n:::\n\n\n### Boosting\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom.seed(3)\n#fixing class outcome labels (0,1,2, work with xgboost as opposed to 1,2,3)\nle = LabelEncoder()\ntest_yb = le.fit_transform(test_y)\n\n#fixing class of binary outcome to work with xgboost\ntest_yb_bin = le.fit_transform(test_y_bin)\n\n#3 Class, Outliers Kept: learning_rate= 0.5, max_depth= 8, n_estimators= 50\nboosting_final_mod= xgb.XGBClassifier(objective='multi:softprob', learning_rate= 0.5, max_depth= 8, n_estimators= 50)\nboosting_final_mod.fit(train_x_st, train_yb)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=8, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=8, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=boosting_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_yb, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9241664606695282\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_yb, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome, Outliers Imputed: learning_rate=0.1, max_depth= 6, n_estimators= 1000\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.97341513 0.8452381  0.95384615]\n```\n:::\n\n```{.python .cell-code}\nboosting_final_mod_bin= xgb.XGBClassifier(objective='binary:logistic', learning_rate= 0.1, max_depth= 6, n_estimators= 1000)\nboosting_final_mod_bin.fit(train_x_st_no_out, train_yb_bin)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=boosting_final_mod_bin.predict(test_x_no_out) #make predictions on test set\n\nf1_1 = f1_score(test_yb_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9474616049738545\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_yb_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.97546012 0.91946309]\n```\n:::\n:::\n\n\n### Support Vector Machines\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom.seed(3)\n#3 Class Outcome, Outliers Kept: C=10, gamma= 0.1, kernel= 'rbf'\nsvm_final_mod=svm.SVC(C=10, gamma= 0.1, kernel= 'rbf')\n\nsvm_final_mod.fit(train_x_st, train_y)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, gamma=0.1)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=svm_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8413941228023978\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_y, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome Outliers Kept: C=10, gamma= 0.1, kernel= 'rbf'\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.95634518 0.78212291 0.78571429]\n```\n:::\n\n```{.python .cell-code}\nsvm_final_mod_bin=svm.SVC(C=10, gamma= 0.1, kernel= 'rbf')\n\nsvm_final_mod_bin.fit(train_x_st, train_y_bin)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, gamma=0.1)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=svm_final_mod_bin.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9124360082897572\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_y_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.9591002  0.86577181]\n```\n:::\n:::\n\n\n### K-Nearest Neighbors\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrandom.seed(3)\n#3 Class, Outliers Kept: n-neighbors= 1\nknn_final_mod=KNeighborsClassifier(n_neighbors=1)\nknn_final_mod.fit(train_x_st, train_y)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=knn_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7923969108297486\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_y, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome, Outliers Imputed: n-neighbors=3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.94512195 0.65895954 0.77310924]\n```\n:::\n\n```{.python .cell-code}\nknn_final_mod_bin=KNeighborsClassifier(n_neighbors=3)\nknn_final_mod_bin.fit(train_x_st_no_out, train_y_bin)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ny_pred=knn_final_mod_bin.predict(test_x_no_out) #make predictions on test set\n\nf1_1 = f1_score(test_y_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8512755510876059\n```\n:::\n\n```{.python .cell-code}\nf1_by_class = f1_score(test_y_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.93612774 0.76642336]\n```\n:::\n:::\n\n\n# Compare Models\n\n**Three-Class F1 Scores (Macro, By-Class):**\n\nLogistic Regression: 0.760 [0.9402229, 0.58682635, 0.75409836]\n\nBoosting: 0.924 [0.97341513, 0.8452381, 0.95384615]\n\nSVM: 0.841 [0.95634518, 0.78212291, 0.78571429]\n\nKNN: 0.792 [0.94512195, 0.65895954, 0.77310924]\n\n**Binary Outcome F1 Scores (Macro, By-Class):**\n\nLogistic Regression: 0.869 [0.94010152 0.79725086]\n\nBoosting: 0.947 [0.97546012, 0.91946309]\n\nSVM: 0.912 [0.9591002, 0.86577181]\n\nKNN: 0.851 [0.93612774 0.76642336]\n\n\nThe best performing model for both a three-class outcome and a binary outcome was the model created with XGBoost. These models actually performed slightly better when trained on the full training set and tested on the held-out test set than they did during cross validation. This is likely because the final model was trained on a larger dataset. Boosting slowly learns through a sequential algorithm where trees are fit on residuals from the previous tree. Because we are fitting many trees, this algorithm is less prone to overfitting. While XGBoost reduces bias and can produce very good predictions, this model is less interpretable than other models such as logistic regression.\n\nLogistic regression is the most biased model used in this analysis. Because this model has low flexibility it could not learn as much from the training data as models like boosting, SVM and KNN can. However, this is likely the most interpretable model from this analysis. While interpretable, the logistic regression model does not perform well enough to be used, especially given performance on the \"suspect\" and \"pathological\" cases.\n\nAfter boosting, the best performing model was support vector machines (SVM). SVM does a better job classifying \"suspect\" and \"pathological\" cases than KNN and logisitic regression, especially on a binary outcome variable. One advantage of SVM is we can impose a softer margin by increasing C (increasing bias), which likely helped SVM learn the nuances of classification of these categories without increasing the variance. SVM also allows us to model highly non-linear relationships (this is likely the case for out data given logistic regression did not perform well). In our case, the optimal kernel was a radial kernel which can perform well in a dataset of higher dimensions.\n\nFinally I tested K-Nearest Neighbors to see if a low bias, high variance model could perform well given our data did not seem to follow a linear relationship. This was not the case, KNN performed similarly to logistic regression, suggesting a model that has a balance between bias and variance would work better with our data. KNN is also not very interpretable. KNN had the lowest score of all the models on the test set for the binary outcome class (and also a low score for the three class outcome) thus, it was likely overfitting to the training data (especially given the optimized k for the 3 class outcome was 1).\n\n# Ethical Implications\n\nWhile I was hoping to land on an interpretable model so one could easily interpret what may be flagging a case as \"suspect\" or \"pathological\", in the case of CTGs, a better performing model is more important than an interpretable model.\n\nWhile boosted models are not very interpretable, the performance is the best for both a 3-class outcome variable and a binary outcome variable.\n\nIn an outpatient setting, where there is more time to take a closer look at the CTGs, I might suggest using the boosted model with a binary outcome (macro f1= 0.947, f1 for class \"normal\"= 0.975, f1 for class \"suspect/pathological\"= 0.919\"). This is because the \"suspect\" case is still difficult to detect in the three-class outcome, so the better f1 score for the combined outcome might be better for flagging suspect cases in addition to pathological cases. In an outpatient setting, the medical professional can then look closer at the flagged cases, interpret why they are being flagged and then class them as \"suspect\" or \"pathological\". For a labor setting, time is more critical, and immediate flagging of pathological cases is more critical. In a labor setting, I would suggest using the boosted 3-class outcome since it performs better on the pathological cases (and suspect cases are not necessarily in need of immediate attention) (macro f1=0.924, f1 for \"normal\"= 0.973, f1 for \"suspect\"= 0.845, and f1 for \"pathological\"= 0.954).\n\nBecause in general, f1 scores for suspect and pathological classes are much lower than for normal, I am not sure I would ever feel comfortable with this model being deployed without a human medical professional also analyzing the CTG and making a decision for themselves. Finally, before being deployed, such a model should be trained on a much larger data set than was done in this analysis (to avoid overfitting).\n\nWhile my original aim was to come up with a high-performing interpretable model, given the low f1 score for suspect/pathological classes, I would consider training a neural network/deep learning model in the future if such a model is able to achieve better performance. However, even if a much higher performing, neural network or other machine learning model is developed, it should not be deployed without people trained to read CTGs analyzing the document for themselves given the low interpretablilty of such an algorithm.\n\n# References\n\nDataset: Ayres de Campos et al. (2000) SisPorto 2.0 A Program for Automated Analysis of Cardiotocograms. J Matern Fetal Med 5:311-318 ([https://onlinelibrary.wiley.com/doi/10.1002/1520-6661(200009/10)9:5%3C311](https://onlinelibrary.wiley.com/doi/10.1002/1520-6661(200009/10)9:5%3C311){.uri}::AID-MFM12%3E3.0.CO;2-9)\n\nCariotocography. (Accessed May 24, 2023). Wikipedia. <https://en.wikipedia.org/wiki/Cardiotocography>. Fetal Health Classification. (Accessed May 24, 2023).",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}