[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Resume",
    "section": "",
    "text": "Northampton, MA\n978-877-6257\nerasmussen@umass.edu\n\n\nR, ArcGIS (familiar with), Microsoft Office (Word, Excel, PowerPoint, Outlook), Google Suites (Docs, Slides, Forms, Sheets)\n\n\n\nUniversity of Massachusetts Amherst, Amherst, MA Expected Graduation: May 2024\nMaster of Science in Data Analytics and Computational Social Sciences (Candidate)\nRelevant Coursework: Data Science Fundamentals, Introduction to Quantitative Analysis, Survey Research Methods\nRoger Williams University, Bristol, RI; May 2021\nBachelor of Science in Public Health; Double Major: Anthropology/Sociology; Minor: Biology\nGPA: 3.949; Honors: Summa cum laude; Honors Program\n2020 Academic Excellence Award in Anthropology/Sociology\n2021 Thomas Sorger Award for Interdisciplinary Excellence in Public Health\n\n\n\nPublic Health Senior Capstone Fall 2020\n“Increasing Staff Retention in the Disability Services Sector”\n\nIdentified public health problems at a human services organization and applied evidence-based research to make recommendations to reduce burnout and increase staff retention\n\nAnthropology/Sociology Senior Thesis Spring 2020\n“American Conceptions of Nutritional Health: A Cross-Cultural Comparison of Food-Based Dietary Guidelines”\n\nConducted a content analysis focusing on the effects of culture on nutrition recommendations\nAwarded Thesis with Distinction\n\n\n\n\nFitzwilly’s, Northampton, MA, Server; June 2022- Present\n\nGreeted guests, assisted customers using menu knowledge to inform orders and ensured customers had a good experience\nServed parties of over twenty guests\n\nBoston University School of Medicine, Framingham, MA, Research Assistant; October 2021- June 2022\n\nConducted research for the Brain Aging Program at the Framingham Heart Study\nReviewed participant medical charts scanning for evidence of cognitive decline and traumatic brain injuries\nRecorded data from participant medical charts using REDcap\n\nRail Trail Flatbread Co., Hudson, MA, Server and Barback; September 2021- June 2022\n\nServed guests and prepared beers, wines, and cocktails as a barback. Used judgement to prioritize tasks during busy periods\nAssisted different areas of the restaurant as needed, including host stand, bussing, and food running\nAttended bi-weekly server meetings and open book management classes to improve productivity as a team\n\nBolton Bean, Bolton MA, Barista; July 2020- October 2021\n\nPrepared coffee and espresso beverages, took orders, assisted customers and helped train new staff\n\nSeven Hills Foundation, Worcester, MA, Human Resources Intern; September 2020- December 2020\n\nPerformed administrative tasks such as filing employee records, helping with onboarding and new employee orientation, performing driving record checks, distributing staff awards, and completing employment verifications\nPerformed receptionist duties including directing visitors and phone calls and enforced COVID-19 sanitation procedures\n\nChartwells K-12 Dining Services, Newport, RI, Student Wellness Intern; September 2019- December 2019\n\nStudied USDA requirements for school lunches, conducted interactive programs for students and their parents, served school lunch following portion requirements, created training presentation for Child and Adult Care Food Program, created a lunch preferences survey for students\n\nLlandudno Pier, Llandudno, Wales, UK, Café Attendant and Bartender; June 2019- August 2019\n\nOperated espresso machine to prepare coffee beverages, prepared food, cleaned, opened and closed café\nPrepared and served alcoholic beverages, used judgement to appropriately card customers, cleaned bar facility\n\nRWU Science Center,Bristol, RI, Biology and Chemistry Tutor; September 2018- September 2019\n\nAssisted peers with homework, labs, and exam preparation\nTaught students how to display data in Excel charts and graphs\n\nRWU Health and Wellness Education, Bristol, RI, Health and Wellness Educator; August 2018- May 2019\n\nOrganized on-campus campaigns to promote healthy lifestyle and behavior choices for students\nCreated surveys and collected feedback to improve future programs\nTrained by NIH-funded Peer Body Project researcher to facilitate small groups of women on the Peer Body Project to prevent eating disorders and promote positive body image at RWU\n\nALS Therapy Development Institute, Cambridge, MA, Tri-State Trek Intern; June 2018\n\nPrepared for large scale bike event, took inventory, and marked course from Boston, MA to Greenwich, CT"
  },
  {
    "objectID": "posts/Post1/index.html",
    "href": "posts/Post1/index.html",
    "title": "Data Science Fundamentals Final Project",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(googlesheets4)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(plotly)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\n\nIntroduction\nIt seems as though every week, or every day, there is another mass shooting at the top of news headlines. While many have tried to explain what it is that causes this seeming increase in mass shootings in the United States (U.S.), from violence in video games, lack of gun control, or the mental health crisis, the picture of what is going on is still unclear. For this project, I am interested in trends in U.S. mass shootings, for instance, when and where they are most likely to occur, and hypothesizing why these trends occur.\nTo conduct this exploratory data analysis, I used mass shooting data from the Gun Violence Archive (GVA). Since 2014, the Gun Violence Archive has manually compiled a ledger of incidences of gun violence in the U.S. (Gun Violence Archive, 2022b). To compile these lists, the GVA triangulates “law enforcement, government, and media sources” (Gun Violence Archive, 2022b). As there is no standard definition for mass shootings, for this project I used the GVA’s definition of mass shooting; four or more people (not including the perpetrator) are shot or killed (Gun Violence Archive, 2022b).\nI joined 9 data sets from the GVA, one for each year 2014 to 2022. The 2022 data included in this analysis goes up until August 27, 2022, the day I read in the data set.\nBecause of the potential of reporting error over the years/media sensationalizing of shooting incidences over time, generally, I will avoid looking at trends over the years and look at trends across months of the year.\n\n\nReading In the Data\n\ngs4_deauth()\n\n#creating a vector of new column names\nmass_names<- c(\"incident_id\", \"incident_date\", \"state\", \"city_or_county\", \"address\", \"number_killed\", \"number_injured\", \"delete\")\n\n#creating a function to read in the data sets with new column names, skip the first row, remove the \"operation\" column which contains links to news articles in the original data source, and create a \"Year\" column for ease of analysis\nread_shootings<-function(sheet_name){read_sheet(\"https://docs.google.com/spreadsheets/d/1rCnIYPQSkcZDCulp5KXAxmZUBad4QtrERi4_7tUMXqs/edit?usp=sharing\", \n                                                sheet=sheet_name,\n                                                col_names=mass_names, \n                                                skip=1) %>%\n    mutate(\"YearSheet\"=sheet_name) %>% \n    mutate(Year=recode(YearSheet, \"MassShootings2014\"=\"2014\", \"MassShootings2015\"=\"2015\", \"MassShootings2016\"=\"2016\", \"MassShootings2017\"=\"2017\", \"MassShootings2018\"=\"2018\", \"MassShootings2019\"=\"2019\", \"MassShootings2020\"=\"2020\", \"MassShootings2021\"=\"2021\", \"MassShootings2022\"=\"2022\")) %>% \n  select(-delete, -YearSheet)\n           }\n\n#using purrr/map_dfr to join data sheets for 2014 through 2022, applying the function read_shootings for consistent formatting\nmass_shootings_all <- map_dfr(\n  sheet_names(\"https://docs.google.com/spreadsheets/d/1rCnIYPQSkcZDCulp5KXAxmZUBad4QtrERi4_7tUMXqs/edit?usp=sharing\")[1:9],\n  read_shootings)\n\nmass_shootings_all\n\n# A tibble: 3,835 × 8\n   incident_id incident_date       state   city_…¹ address numbe…² numbe…³ Year \n         <dbl> <dttm>              <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>\n 1      271363 2014-12-29 00:00:00 Louisi… New Or… Poydra…       0       4 2014 \n 2      269679 2014-12-27 00:00:00 Califo… Los An… 8800 b…       1       3 2014 \n 3      270036 2014-12-27 00:00:00 Califo… Sacram… 4000 b…       0       4 2014 \n 4      269167 2014-12-26 00:00:00 Illino… East S… 2500 b…       1       3 2014 \n 5      268598 2014-12-24 00:00:00 Missou… Saint … 18th a…       1       3 2014 \n 6      267792 2014-12-23 00:00:00 Kentuc… Winche… 260 Ox…       1       3 2014 \n 7      268282 2014-12-22 00:00:00 Michig… Detroit Charle…       1       3 2014 \n 8      282186 2014-12-22 00:00:00 New Yo… Webster 191 La…       4       2 2014 \n 9      267721 2014-12-22 00:00:00 Illino… Chicago 5700 b…       0       5 2014 \n10      266570 2014-12-21 00:00:00 Florida Saraso… 4034 N…       2       2 2014 \n# … with 3,825 more rows, and abbreviated variable names ¹​city_or_county,\n#   ²​number_killed, ³​number_injured\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAfter joining the sheets, I double checked that the number of rows in the google sheets added up to the number of rows in my joined data set above (minus 9 for the column names).\n\n\nTidying the Data\nThe initial data set includes an incident ID, the date in POSIXct format, state, city or county, the address of the incident, number killed, number injured, as well as the “Year” column I created from the sheet names. The data is fairly tidy to begin with, each “case” is a particular shooting incident (uniquely defined by the incident ID) and has its own row, each variable (characteristic of the shooting) has its own column, and each value has it’s own cell. I still need to change the state column into “date” format, add a “month” column to use for analysis, and change the order of the columns to make more logical sense.\n\n#converting incident_date from \"POSIXct\" to \"date\" format\nmass_shootings_all$incident_date<-as.Date(mass_shootings_all$incident_date)\n\n#creating a month column and converting to factors\nmass_shootings_all<-mass_shootings_all%>% \n  mutate(month=as.factor(month(incident_date))) %>% \n    mutate(month=recode(month, `1`=\"Jan\", `2`=\"Feb\", `3`=\"Mar\", `4`=\"Apr\", `5`=\"May\", `6`=\"Jun\", `7`=\"Jul\", `8`=\"Aug\", `9`=\"Sept\", `10`=\"Oct\", `11`=\"Nov\", `12`=\"Dec\"))\n\n#reordering the columns \nmass_shootings_all<-mass_shootings_all %>% \n  select(c(\"incident_id\", \"incident_date\", \"Year\", \"month\", \"state\", \"city_or_county\", \"address\", \"number_killed\", \"number_injured\"))\nmass_shootings_all\n\n# A tibble: 3,835 × 9\n   incident_id incident_date Year  month state   city_…¹ address numbe…² numbe…³\n         <dbl> <date>        <chr> <fct> <chr>   <chr>   <chr>     <dbl>   <dbl>\n 1      271363 2014-12-29    2014  Dec   Louisi… New Or… Poydra…       0       4\n 2      269679 2014-12-27    2014  Dec   Califo… Los An… 8800 b…       1       3\n 3      270036 2014-12-27    2014  Dec   Califo… Sacram… 4000 b…       0       4\n 4      269167 2014-12-26    2014  Dec   Illino… East S… 2500 b…       1       3\n 5      268598 2014-12-24    2014  Dec   Missou… Saint … 18th a…       1       3\n 6      267792 2014-12-23    2014  Dec   Kentuc… Winche… 260 Ox…       1       3\n 7      268282 2014-12-22    2014  Dec   Michig… Detroit Charle…       1       3\n 8      282186 2014-12-22    2014  Dec   New Yo… Webster 191 La…       4       2\n 9      267721 2014-12-22    2014  Dec   Illino… Chicago 5700 b…       0       5\n10      266570 2014-12-21    2014  Dec   Florida Saraso… 4034 N…       2       2\n# … with 3,825 more rows, and abbreviated variable names ¹​city_or_county,\n#   ²​number_killed, ³​number_injured\n# ℹ Use `print(n = ...)` to see more rows\n\n#saving an original copy of the tidied data set\nmass_shootings_orig<-mass_shootings_all\n\n\n\nCreating Initial Plots by Year and Month\nAs mentioned in my introduction, I am interested in trends in mass shootings over time. To begin, I will look at how the number of mass shootings has changed by year across my data set, and then look at how the number of mass shootings varies across months of the year.\n\n#creating histogram of shootings/year\nggplot(mass_shootings_all, aes(Year))+\n  geom_bar(stat=\"Count\")+\n  labs(title=\"Figure 1: Number of Mass Shootings 2014-2022*\", caption=\"*2022 data goes up to August 27, 2022\", x=\"Month\", y=\"Count\")\n\n\n\n#creating line plot by year and month\nggplot(mass_shootings_all, aes(x=month, group=Year, color=Year))+\n  geom_line(stat=\"count\")+\n  geom_point(stat=\"count\")+\n  labs(title=\"Figure 2: Number of Mass Shootings by Month 2014-2022*\", caption=\"*2022 data is only up until August 27, 2022\", x=\"Month\", y=\"Count\")\n\n\n\n#excluding 2022 from the rest of the graphs so as to not affect the month distribution (for which we don't yet have Sept-Dec data)\nmass_shootings_all<-filter(mass_shootings_all, Year!=2022)\n\n#creating plot by month\nggplot(mass_shootings_all, aes(x=month))+\n  geom_point(stat=\"count\")+geom_line(stat=\"count\", group=1)+\n  labs(title=\"Figure 3: Number of Mass Shootings By Month (2014-2021)\", x=\"Month\", y=\"Count\")\n\n\n\n\nFigure 1 : I will not spend to much time analyzing this graph because of the issue I mentioned in the introduction. However, if mass shootings have accurately been reported/compiled in this data set, it does appear that mass shootings have generally been increasing since 2014, but especially in 2019, with a pretty large jump in 2020. This is surprising to me based on my experience with the news in 2020. I remember feeling like I was hearing of less shootings in 2020 than 2019. This data paints a different picture. One possible explanation could be that the media was more consumed with reporting COVID-19-related news during 2020.\nFigure 2 : This graph actually expands on my thoughts about shootings in 2020. The 2020 line shows a massive increase in May and June 2020. This is right after the intial COVID-19 stay at home orders/guidelines in much of the U.S. in March 2020. Could this be, for lack of a better word, “pent-up demand” for mass shootings? There were less gatherings/ opportunities for mass shootings given the circumstances in March 2020, and perhaps more of an opportunity as restrictions relaxed in the summer. At the same time, it appears that most years also show an increase in shootings in June and July.\nFigure 3 : This graph creates a more jarring picture of the possible seasonality of mass shootings. It appears that the most occur in June and July, with incidences increasing beginning in April and tapering down beginning in August.\nAfter looking at these initial graphs I am curious, why do the most mass shootings occur in June and July? Is it temperature/seasonally dependent? According to a report from the U.S. Department of Justice, many criminal activities actually follow seasonal variations (2014). The report found that all types of criminal victimization in the study (property victimization, and other forms of violent crime) except robbery followed the trend of more crimes in the summer versus the winter (U.S. Department of Justice, 2014). However, the study did not provide explanations as to why this is the case (U.S. Department of Justice 2014).\nMoving on, I am curious if mass shootings also follow these seasonal trends. I am also curious: if violence is temperature dependent, would states with more/less seasonal temperature variation follow the national trend? Below I look at FL and IL and create a plot for all states to see how well they follow the national trends:\n\n\nCreating Plots by State\n\n#Distribution of shootings by month in FL\nfilter(mass_shootings_all, state==\"Florida\") %>% \n    ggplot(aes(month))+geom_point(stat=\"Count\")+geom_line(stat=\"count\", group=1) +labs(title=\"Figure 4: Number of Mass Shootings By Month in Florida (2014-2021)\", x=\"Month\", y=\"Count\")\n\n\n\n#Distribution of shootings by month in MA\nfilter(mass_shootings_all, state==\"Massachusetts\") %>% \n    ggplot(aes(month))+geom_point(stat=\"Count\")+geom_line(stat=\"count\", group=1)+labs(title=\"Figure 5: Mass Shootings By Month in Massachusetts (2014-2021)\", x=\"Month\", y=\"Count\")\n\n\n\n#Mass has some months where count=0, which is omitted from the histogram when filtering out mass. Below I created a table and then created a bar graph from this to preserve the months where count=0\nmass_shootings_all_mass<-filter(mass_shootings_all, state==\"Massachusetts\") %>% \n    group_by(month, .drop=FALSE) %>%\n    summarise(Count = n())\nmass_shootings_all_mass\n\n# A tibble: 12 × 2\n   month Count\n   <fct> <int>\n 1 Jan       2\n 2 Feb       0\n 3 Mar       1\n 4 Apr       2\n 5 May       4\n 6 Jun       5\n 7 Jul       6\n 8 Aug       6\n 9 Sept      0\n10 Oct       3\n11 Nov       0\n12 Dec       2\n\nggplot(mass_shootings_all_mass, aes(x=month, y=Count))+geom_point(stat=\"identity\")+geom_line(stat=\"identity\", group=1)+labs(title=\"Figure 6: Mass Shootings By Month in Massachusetts (2014-2021)\", x=\"Month\", y=\"Count\")\n\n\n\n#Distribution of shootings by month in IL (better example of distribution than Mass given higher numbers of shootings)\nfilter(mass_shootings_all, state==\"Illinois\") %>% \n    ggplot(aes(month))+geom_point(stat=\"Count\")+geom_line(stat=\"count\", group=1)+labs(title=\"Figure 7: Mass Shootings By Month in Illinois (2014-2021)\", x=\"Month\", y=\"Count\")\n\n\n\n#creating month distribution by state. This table DOES NOT preserve the months where the count=0 but easier to visualize patterns than with histogram\n#remove x-axis values to reduce clutter\nggplot(mass_shootings_all, aes(month))+geom_line(stat=\"count\", group=1)+ facet_wrap(~state, scales = \"free_y\")+theme(strip.text = element_text(size=6))+labs(title=\"Figure 8: Mass Shootings by Month Across U.S. States (2014-2021)\")+theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n\n\n\n#creating month distribution by state. This table DOES preserve the months where the count=0\nggplot(mass_shootings_all, aes(month))+geom_histogram(stat=\"count\")+ facet_wrap(~state, scales = \"free_y\")+theme(strip.text = element_text(size=6))+labs(title=\"Figure 9: Mass Shootings by Month Across U.S. States (2014-2021)\", x=\"Month\", y=\"Count\")+theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n\n\n\n\nFigure 4 : Florida does not appear to follow the national trend in mass shootings mainly occurring during the summer.\nFigure 5 : Filtered data set does not have rows for months where the count of mass shootings is equal to zero. I remake this graph in Figure 6.\nFigure 6 : Corrected MA graph which preserves months where count=0. Seems to more closely follow the national trend.\nFigure 7 : Illinois might provide a better example to analyze seasonal variation than MA given the larger number of mass shootings in IL/larger population. The distribution of mass shootings by month in IL very closely follows the national trend.\nFigure 8 : Line graph for every U.S. state. Unfortunately, we run into the same problem as for MA, months where count=0 for particular states are omitted from the graph.\nFigure 9 : To more easily see what is going on at the month level, I include a histogram which preserves months where the count=0. There is a wide variety in distribution of shootings by month based on state. However, it appears that states which are generally hotter year round (FL, TN, LA) do not tend to conform to the national trend as well, while states with more seasonal weather variation (MA, NY, IL) more closely follow the national trend.\n\n\nCreating a New Variable\nThere definitely appears to be a correlation between season on the occurrence of mass shootings. If there are more mass shootings based on opportunity (i.e. more outdoor gatherings, people are more likely to leave the house) in the summer, we would expect more shootings to occur outside during the summer. If it is based on irritability due to the heat, we might not see a big difference in shootings occurring inside versus outside in the summer versus winter. In other words do most shootings during summer months occur outside? Are there more shootings in July because they are occurring outside, because there are more outdoor gatherings?\nBelow I create a new variable “location” predicting whether the shooting occurred “inside” or “outside” based on the wording of the address the shooting occurred at.\n\n#creating a new variable column \"location\" to denote if incident was likely inside or outside based on address column. detects presence of \"outside words\" or if a address string starts with a number.\noutside_words<-c(\"block\", \"Block\", \"corner\", \"and\", \"of\")\nmass_shootings_all<-mass_shootings_all%>%\n  mutate(location=case_when(\n    str_detect(address,paste0(outside_words, collapse=\"|\"))~ \"outside\",\n    str_starts(address, \"[0-9]\") ~ \"inside\",\n    TRUE ~ \"outside\"))\n\n#New column location versus addresses\nmass_shootings_all %>% \n  select(address, location)\n\n# A tibble: 3,393 × 2\n   address                             location\n   <chr>                               <chr>   \n 1 Poydras and Bolivar                 outside \n 2 8800 block of South Figueroa Street outside \n 3 4000 block of May Street            outside \n 4 2500 block of Summit Avenue         outside \n 5 18th and Pine                       outside \n 6 260 Oxford Drive                    inside  \n 7 Charlevoix and Philip               outside \n 8 191 Lake Rd                         inside  \n 9 5700 block of South Green Street    outside \n10 4034 N Washington Blvd              inside  \n# … with 3,383 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n#Plotting inside versus outside shootings by month\nggplot(mass_shootings_all, aes(x=month, group=location, color=location))+geom_point(stat=\"count\")+geom_line(stat=\"count\")+labs(title=\"Figure 10: Location of Mass Shootings by Month (2014-2021)\")\n\n\n\n#how does COVID-19/years affect the number inside versus outside shootings\nggplot(mass_shootings_all, aes(x=Year, group=location, color=location))+geom_point(stat=\"count\")+geom_line(stat=\"count\")+labs(title=\"Figure 11: Location of Mass Shootings by Year (2014-2022)\")\n\n\n\n\nFigure 10 : It appears that there are significantly more shootings occurring outside during the summer months, however, indoor shootings too increase during the summer months, suggesting that multiple factors (besides opportunity) could be contributing to the increase in shootings during the summer months.\nFigure 11 : While the indoor/outdoor lines are generally very similar, it is interesting looking at the disparity in indoor/outdoor shootings in 2020, as it appears there are significantly more outside shootings in 2020, possibly given offices, schools, and other “indoor” settings were closed or at limited capacity for much of the year.\n\n\nDoes the Severity of Shootings Correlate with Seasonality?\nNext, I will analyze how many people are shot or killed during mass shootings, and if seasonality also affects severity.\nFirst I will create a new variable, “number_shot”, because I believe this a better measure of severity of a shooting than number killed or number injured alone. Number_shot is a better measure of intention of the shooter (how many people the shooter intended to kill/affect), however this is still a limited measure as skill/how good the shooters aim is may affect if he/she makes their intended impact.\nBecause there is also large variability in the number shot, I created a new variable, severity, with categories based on the number of people shot (low, mid, and high severity). I chose <9 shot as low severity 10-29 as mid severity, and 30+ as high severity based on the number of distinct values of number_shot. This variable is still difficult to work with, given the majority of mass shootings only involve four victims. However, it is useful for weeding out the lower severity shootings to see trends in the higher severity shootings where 10 or more people are killed. I chose 30+ as high severity as there are few with over 30 people shot so this seemed like a logical threshold for high severity (as they also go way beyond 30 shot, one incident, involving 500 people shot).\n\n#creating a new column/variable to measure severity based on above variables, number_shot= number_killed+number_injured\nmass_shootings_all<-mass_shootings_all %>% \n  mutate(number_shot= number_injured+number_killed)\n\n#Looking at distinct values for new variable\ndistinct(mass_shootings_all, number_shot) %>% \n  arrange(number_shot)\n\n# A tibble: 29 × 1\n   number_shot\n         <dbl>\n 1           4\n 2           5\n 3           6\n 4           7\n 5           8\n 6           9\n 7          10\n 8          11\n 9          12\n10          13\n# … with 19 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n#creating a new variable, severity, by categorizing the number shot into low, mid, and high severity\nmass_shootings_all<-mass_shootings_all %>% \n  mutate(severity= case_when(number_shot <= 9 ~ \"low\", \n                             number_shot >= 10 & number_shot <= 29 ~ \"mid\", \n                             number_shot >= 30 ~ \"high\"))\nmass_shootings_all\n\n# A tibble: 3,393 × 12\n   incide…¹ incident…² Year  month state city_…³ address numbe…⁴ numbe…⁵ locat…⁶\n      <dbl> <date>     <chr> <fct> <chr> <chr>   <chr>     <dbl>   <dbl> <chr>  \n 1   271363 2014-12-29 2014  Dec   Loui… New Or… Poydra…       0       4 outside\n 2   269679 2014-12-27 2014  Dec   Cali… Los An… 8800 b…       1       3 outside\n 3   270036 2014-12-27 2014  Dec   Cali… Sacram… 4000 b…       0       4 outside\n 4   269167 2014-12-26 2014  Dec   Illi… East S… 2500 b…       1       3 outside\n 5   268598 2014-12-24 2014  Dec   Miss… Saint … 18th a…       1       3 outside\n 6   267792 2014-12-23 2014  Dec   Kent… Winche… 260 Ox…       1       3 inside \n 7   268282 2014-12-22 2014  Dec   Mich… Detroit Charle…       1       3 outside\n 8   282186 2014-12-22 2014  Dec   New … Webster 191 La…       4       2 inside \n 9   267721 2014-12-22 2014  Dec   Illi… Chicago 5700 b…       0       5 outside\n10   266570 2014-12-21 2014  Dec   Flor… Saraso… 4034 N…       2       2 inside \n# … with 3,383 more rows, 2 more variables: number_shot <dbl>, severity <chr>,\n#   and abbreviated variable names ¹​incident_id, ²​incident_date,\n#   ³​city_or_county, ⁴​number_killed, ⁵​number_injured, ⁶​location\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n#2D histogram, depicting incidents by year based on number killed, and a count for how many incidents in a particular year\nmass_shootings_all %>% \n  filter(number_shot<100) %>% \n  ggplot(aes(Year, number_shot))+geom_bin2d()+labs(title=\"Figure 12: Mass Shootings by Number of People Killed or Injured (2014-2021)*\", caption=\"*Shootings with over 100 killed/injured are exluded for readability of plot\", y=\"Number Shot\")\n\n\n\n#Creating line graph for severity\nmass_shootings_all %>% \n  ggplot(aes(x=month, group=severity, color=severity))+geom_point(stat=\"count\")+geom_line(stat=\"count\")+labs(title=\"Figure 13: Severity of Mass Shootings by Month (2014-2021)\", x=\"Month\", y=\"Count\")\n\n\n\n#removing \"low\" from severity line graph to more easily visualize trends for mid and high severity shootings\nmass_shootings_all %>% \n  ggplot(aes(x=month, group=severity, color=severity))+geom_point(stat=\"count\")+geom_line(stat=\"count\")+labs(title=\"Figure 14: Severity of Mass Shootings by Month (2014-2021)*\", caption=\"*Low severity shootings excluded from this graph for readability\", x=\"Month\", y=\"Count\")+ylim(0, 20)\n\n\n\n#Totaling the number of people killed in mass shootings by month\nmass_shootings_all_month_sum<-mass_shootings_all%>% \n    group_by(month) %>%\n    summarise(total_shot=sum(number_shot))\n\n#Plot based on above sum total\nggplot(mass_shootings_all_month_sum)+\n  geom_line(aes(x=month, y=total_shot, group=1))+ geom_point(aes(x=month, y=total_shot, group=1))+labs(title=\"Figure 15: Total Number of People Shot or Killed in Mass Shootings by Month (2014-2021)\", x=\"Month\", y=\"Total Shot\")\n\n\n\n#Creating interacting scatter plot with ggplot-ly\np1<-filter(mass_shootings_all, number_shot<100) %>% \n  ggplot(aes(x=incident_date, y=number_shot))+geom_point(size=0.8)+labs(title=\"Figure 16: Shootings 2014-2021\", x=\"Incident Date\", y=\"Total Shot\")\n\np1<-ggplotly(p1)\np1\n\n\n\n\n\nFigure 12 : this 2d plot helps depict trends in the number shot over time. For instance in 2020-2021, there appears to be a higher count of shootings where less than 10 people are shot, however there does not appear to be an increase in shooting with a higher number of people shot over time.\nFigure 13 : This graph shows that low severity shootings appear to follow seasonal trends, but it is difficult to visualize low and mid severity shootings on this graph.\nFigure 14 : This graph filters out mid and high severity shootings across months. Mid severity shootings seem to somewhat follow the national seasonality trend, but there does not seem to be much correlation with seasonality for the high severity shootings. However, we have fewer points in the high severity group, so again, this is difficult to draw conclusions from. From these graphs, it seems that low severity shootings follow the seasonality trend, but mid and high severity shootings may be less likely to conform to this trend.\nFigure 15 : To look at severity, I also created a graph with the sum of individuals killed in mass shootings by month. Because the graph is not more “stretched out”/“taller, it seems that shootings occurring in the summer do not necessarily include more victims, there are just more shootings overall. The spike in October is likely due to the 2017 shooting in Las Vegas that killed 59 and injure 441 people on October 1st.\nFigure 16 :Interactive plot with the number shot and the date. It is interesting to hone in on a particular incident and view when it occurred and how many people were affected. Shootings with over 100 killed/injured are excluded for readability of plot. 2 missing points: October 1, 2017, where 500 people were shot in Las Vegas, and June 12, 2016, where 103 people were shot in Orlando.\n\n\nConclusion\nOverall it appears there is a seasonal impact on the number of mass shootings, but not necessarily the severity of mass shootings. June and July seem to have the greatest incidence of mass shootings. It is difficult to explain why more mass shootings occur during the summer months but opportunity is likely a contributing factor, given that more mass shootings occur outside during the summer versus inside. Opportunity also appears to be at play when analyzing mass shooting trends in the year 2020, as there is a massive increase in mass shootings in May and June following the lifting of heavy COVID-19 restrictions, and a much larger amount of outdoor compared to indoor shootings in 2020, perhaps due to a reduction of indoor gatherings during this year. Overall, it appears that mass shootings follow the seasonality trend of other violent crimes.\nThis analysis leaves me with many questions and thoughts about future research. For instance, it would be interesting to join this data with more information on the location, for instance if it occurred in a home, an office, an event, a school, or on the street and perhaps seeing if the distribution of shootings in each month are seasonally correlated at the setting level or perhaps there are other factors involved. I am also interested to see if particular events correlate with the timing of shootings (for instance the January 6 capitol attack, the war in Ukraine, or other national and international events). Finally, I am curious whether motive is correlated with seasonality, for instance are more spontaneous shootings likely to occur during the summer and more premeditated crimes less likely to follow this trend?\n\n\nReferences\nGun Violence Archive (2014). Mass Shootings - 2014. https://www.gunviolencearchive.org/reports/mass-shootings/2014\nGun Violence Archive (2015). Mass Shootings - 2015. https://www.gunviolencearchive.org/reports/mass-shootings/2015\nGun Violence Archive (2016). Mass Shootings - 2016. https://www.gunviolencearchive.org/reports/mass-shooting?year=2016\nGun Violence Archive (2017). Mass Shootings - 2017. https://www.gunviolencearchive.org/reports/mass-shooting?year=2017\nGun Violence Archive (2018). Mass Shootings - 2018. https://www.gunviolencearchive.org/reports/mass-shooting?year=2018\nGun Violence Archive (2019). Mass Shootings - 2019. https://www.gunviolencearchive.org/reports/mass-shooting?year=2019\nGun Violence Archive (2020). Mass Shootings - 2020. https://www.gunviolencearchive.org/reports/mass-shooting?year=2020\nGun Violence Archive (2021). Mass Shootings - 2021. https://www.gunviolencearchive.org/reports/mass-shooting?year=2021\nGun Violence Archive (2022a). Mass Shootings - 2022. https://www.gunviolencearchive.org/reports/mass-shooting\nGun Violence Archive. (2022b, January 3). General Methodology. https://www.gunviolencearchive.org/methodology\nLauritsen, Janet J. and Nicole White. (2014). “Seasonal Patterns in Criminal Victimization Trends”. (June 2014). U.S. Department of Justice. 1-22. https://bjs.ojp.gov/content/pub/pdf/spcvt.pdf\nWickham, Hadley and Garrett Grolemund. (2017). R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Quantitative Analysis Final Project",
    "section": "",
    "text": "Political Affiliation and COVID-19: A County-Level Analysis of Political Partisanship on COVID-19 Death Rates"
  },
  {
    "objectID": "posts/Post 3/index.html",
    "href": "posts/Post 3/index.html",
    "title": "Machine Learning Final Project",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(VIM)\nlibrary(lubridate)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(e1071)\nlibrary(class)\nlibrary(nnet)\nlibrary(boot)\nlibrary(caret)\nlibrary(MLmetrics)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(fastDummies)\n\nvirtualenv_create(\"MLFinal\") #create virtual environment... per reticulate cheat sheet\n\nvirtualenv: MLFinal\n\npy_install(\"pandas\", env_name=\"MLFinal\")\npy_install(\"numpy\", env_name=\"MLFinal\")\npy_install(\"xgboost\", env_name=\"MLFinal\")\nconda_install(\"MLFinal\", \"scikit-learn\")\n\nuse_virtualenv(\"MLFinal\")\n\nknitr::knit_engines$set(python =\nreticulate::eng_python)"
  },
  {
    "objectID": "posts/Post 3/index.html#creating-dummy-variables-from-histogram-tendency-variable",
    "href": "posts/Post 3/index.html#creating-dummy-variables-from-histogram-tendency-variable",
    "title": "Machine Learning Final Project",
    "section": "Creating Dummy Variables from Histogram Tendency Variable",
    "text": "Creating Dummy Variables from Histogram Tendency Variable\nSince analysis will be done in python, I need categorical variables coded as dummy variables.\n\nfet_df<-mutate(fet_df, histogram_tendency=(case_when(\n                                   histogram_tendency == -1 ~ \"negative\",\n                                   histogram_tendency == 0 ~ \"zero\",\n                                   histogram_tendency == 1 ~ \"positive\")))\n\n#using fastDummies package\nfet_df<- dummy_cols(fet_df, select_columns = 'histogram_tendency')\nhead(dplyr::select(fet_df, 'histogram_tendency_negative', 'histogram_tendency_positive', 'histogram_tendency_zero'), 5)\n\n  histogram_tendency_negative histogram_tendency_positive\n1                           0                           1\n2                           0                           0\n3                           0                           0\n4                           0                           1\n5                           0                           1\n  histogram_tendency_zero\n1                       0\n2                       1\n3                       1\n4                       0\n5                       0"
  },
  {
    "objectID": "posts/Post 3/index.html#creating-a-binary-outcome-variables",
    "href": "posts/Post 3/index.html#creating-a-binary-outcome-variables",
    "title": "Machine Learning Final Project",
    "section": "Creating a Binary Outcome Variables",
    "text": "Creating a Binary Outcome Variables\nAs I ran the models, I realized all models have a difficult time detecting suspect cases in particular. I combined suspect and pathological classes into a binary variable to see if this might improve performance when it comes to detecting these categories.\n\n#mutating outcome to be binary 1= normal, 2= suspect & pathological. New outcome column is called fetal_health_bin\n\n#case_when does not work well with factors, so I converted to characters and then back to factors\nfet_df$fetal_health<-as.character(fet_df$fetal_health)\n\n#Mutate into new binary outcome column fetal_health_bin (0=normal, 1= flagged cases)\nfet_df<-fet_df %>% \n  mutate(fetal_health_bin=case_when(\n  fetal_health== \"1\" ~ \"0\",\n  fetal_health == \"2\" | fetal_health == \"3\" ~ \"1\"))\n\n#fix classes of mutated columns\nfet_df$fetal_health_bin<-as.factor(fet_df$fetal_health_bin)\nfet_df$fetal_health<-as.factor(fet_df$fetal_health)"
  },
  {
    "objectID": "posts/Post 3/index.html#creating-ordered-factors-for-3-class-outcome",
    "href": "posts/Post 3/index.html#creating-ordered-factors-for-3-class-outcome",
    "title": "Machine Learning Final Project",
    "section": "Creating Ordered Factors for 3 Class Outcome:",
    "text": "Creating Ordered Factors for 3 Class Outcome:\nI also tested an ordered factor outcome category throughout runs, but to save space and time running the code I removed this from the analysis. Ordering the outcome factors generally resulted in the same outcome as for unordered classes.\n\n#trying out ordered factors\nfet_df$fetal_health_fac <- factor(fet_df$fetal_health, ordered = TRUE, \n                                levels = c(\"1\", \"2\", \"3\"))\n\n\n#Checking classes of new columns\nclass(fet_df$fetal_health_fac)\n\n[1] \"ordered\" \"factor\" \n\nclass(fet_df$fetal_health)\n\n[1] \"factor\""
  },
  {
    "objectID": "posts/Post 3/index.html#splitting-the-data",
    "href": "posts/Post 3/index.html#splitting-the-data",
    "title": "Machine Learning Final Project",
    "section": "Splitting the Data",
    "text": "Splitting the Data\nI am setting aside the test (hold-out) set for use after cross-validation/ hyperparameter optimization. The training set has 1488 observations (70% of the data) and the test set has 638 observations (30% of the data).\n\n#create ID column\nfet_df$id<- 1:nrow(fet_df)\n\nset.seed(12)\n\n#use 70% of dataset as training set and 30% as test set \ntrain <- fet_df %>% sample_frac(0.7) \ntest <- anti_join(fet_df, train, by = 'id')\n\n#Checking dimensions of split data\ndim(train)\n\n[1] 1488   28\n\ndim(test)\n\n[1] 638  28\n\n\nThe rest of the data preprocessing (scaling and outlier imputing) will be done within each cross-validation split using sci-kit learn’s pipeline."
  },
  {
    "objectID": "posts/Post 3/index.html#penalized-logistic-regression",
    "href": "posts/Post 3/index.html#penalized-logistic-regression",
    "title": "Machine Learning Final Project",
    "section": "Penalized Logistic Regression",
    "text": "Penalized Logistic Regression\n\n#import python modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nimport random\n\n\nTesting l1 and l2 Penalty\n\n#setting parameters to be tested with GridSearchCV\nparams = [\n{'log_l__C': [0.01,0.1,1,10,100,500,1000],\n'log_l__penalty': ['l1', 'l2']} # testing l1 and l2 penalty\n]\n\n#define pipeline\nlog_pipe = Pipeline(steps=[('scale', StandardScaler()), ('log_l', LogisticRegression(solver='saga', tol=0.006))])\n\n#define pipeline that also removes outliers\nlog_pipe_no_out = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('log_l', LogisticRegression(solver='saga', tol=0.006))])\n\n#Apply Grid Search\nGS_log = GridSearchCV(log_pipe, param_grid=params, scoring=\"f1_macro\", cv=5) #with outliers\nGS_log_no_out = GridSearchCV(log_pipe_no_out, param_grid=params, scoring=\"f1_macro\", cv=5) #with outliers imputed\n\nUsing the pipeline should apply transformations at each fold to avoid data leakage and get more accurate evaluation metrics.\n\n\nDefining a Function to Return Macro f1 Scores and By-Class F1 Scores\nBelow I define a function that takes in the outcome variable (y) and pre-defined GridSearchCV object and outputs macro f1 and by-class f1 scores\n\ndef CV_F1_function(GS_object, outcome):\n  \n  random.seed(3) # set seed each time the algorithm is run for reproducibility\n  \n  #print best parameters and macro f1\n  best_GS=GS_object.fit(train_x, outcome) #finds the best parameters\n  print(best_GS.best_params_)\n  print(f'Best Macro F1:  {best_GS.best_score_}')\n  \n  #Print by-class f1 scores \n  pred1 = cross_val_predict(best_GS.best_estimator_, train_x, outcome, cv=5) #plugs in best parameters\n  f1_class = f1_score(outcome, pred1, average=None) #calculates by-class f1 scores\n  print(f1_class)\n\nI use the above function to determine best parameters and optimize macro f1 scores for our data with outliers and with the outliers removed and imputed with the mean (0). This function returns optimized parameters, macro f1 scores, and by-class f1 scores (in the order: normal, suspect, pathological). I use this function throughout the rest of this document.\nPlugging logistic regression grid search objects into function:\n\n\nCV_F1_function(GS_object=GS_log, outcome=train_y) #3 Classes\n\n{'log_l__C': 1, 'log_l__penalty': 'l1'}\nBest Macro F1:  0.8194031154528476\n[0.9524618  0.70952381 0.8       ]\n\nCV_F1_function(GS_object=GS_log_no_out, outcome=train_y) #3 Classes with x variable outliers removed\n\n{'log_l__C': 500, 'log_l__penalty': 'l2'}\nBest Macro F1:  0.7851217297163775\n[0.94282084 0.67990074 0.72641509]\n\nCV_F1_function(GS_object=GS_log, outcome=train_y_bin) #Binary Outcome\n\n{'log_l__C': 1000, 'log_l__penalty': 'l1'}\nBest Macro F1:  0.8584997817538043\n[0.9407282  0.77198697]\n\nCV_F1_function(GS_object=GS_log_no_out, outcome=train_y_bin) #Binary Outcome with x variable outliers removed\n\n{'log_l__C': 10, 'log_l__penalty': 'l1'}\nBest Macro F1:  0.8374660860773042\n[0.93316413 0.74183007]\n\n\nOur best macro f1 score for the 3 class unordered outcome (outliers kept) is 0.8194 where the f1 for “normal” is 0.952, 0.7095 for “suspect”, and 0.8 for “pathological”. The optimized parameters are an l1 penalty and C=1 (C is 1/lambda, so a small C indicates a large penalty).\nFor the binary outcome, out best f1 score is 0.8585 (occurs when outliers are kept) with an f1 for normal of 0.9416 and for the suspect/pathological cases 0.7752. The optimized C is 100, (a smaller penalty than for the 3 class outcome) and once again, the l1 penalty."
  },
  {
    "objectID": "posts/Post 3/index.html#elastic-net-regularization",
    "href": "posts/Post 3/index.html#elastic-net-regularization",
    "title": "Machine Learning Final Project",
    "section": "Elastic Net Regularization",
    "text": "Elastic Net Regularization\n\n\nparams = [\n{'log_net__l1_ratio': [0.2, 0.5, 0.7, 0.8, 0.9, 1], #1 indicates full f1 penalty \n'log_net__C': [0.01, 0.1, 1.0, 10, 100]} # tests 20 values of C between 0 and 4 on the log scale}\n]\n\nlog_net = LogisticRegression(solver='saga', tol=0.006, penalty='elasticnet')\n\nlog_net_pipe = Pipeline(steps=[('scale', StandardScaler()), ('log_net', LogisticRegression(solver='saga', tol=0.006, penalty='elasticnet'))])\n\nlog_net_pipe_no_out = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('log_net', LogisticRegression(solver='saga', tol=0.006, penalty='elasticnet'))])\n\nGS_log_net = GridSearchCV(log_net_pipe, param_grid=params, scoring=\"f1_macro\", cv=5)\nGS_log_net_no_out = GridSearchCV(log_net_pipe_no_out, param_grid=params, scoring=\"f1_macro\", cv=5)\n\n\n\nCV_F1_function(GS_object=GS_log_net, outcome=train_y) #3 Classes\n\n{'log_net__C': 1.0, 'log_net__l1_ratio': 0.9}\nBest Macro F1:  0.8194031154528476\n[0.9520577  0.70644391 0.8       ]\n\nCV_F1_function(GS_object=GS_log_net_no_out, outcome=train_y) #Outliers removed\n\n{'log_net__C': 1.0, 'log_net__l1_ratio': 0.7}\nBest Macro F1:  0.7854264924167683\n[0.94296578 0.68304668 0.73267327]\n\nCV_F1_function(GS_object=GS_log_net, outcome=train_y_bin) #Binary Outcome\n\n{'log_net__C': 100, 'log_net__l1_ratio': 0.2}\nBest Macro F1:  0.8584997817538043\n[0.94157494 0.7752443 ]\n\nCV_F1_function(GS_object=GS_log_net_no_out, outcome=train_y_bin)#Outliers Removed\n\n{'log_net__C': 10, 'log_net__l1_ratio': 0.2}\nBest Macro F1:  0.8374660860773042\n[0.93316413 0.74183007]\n\n\nAgain, our best macro f1 score for the unordered 3 class outcome (outliers kept) is 0.8194. The optimized C is still 1, and the l1 ratio is 0.9, indicating a stronger l1 penalty performs better. This is consistent with the previous runs finding the l1 penalty outperformed the l2 penalty. For the binary outcome, the macro f1 score is 0.8589. This is very close to the results from the previous section. For simplicity’s sake I will only test the optimized parameters with the l1 penalty from the previous section on the final test set."
  },
  {
    "objectID": "posts/Post 3/index.html#boosting",
    "href": "posts/Post 3/index.html#boosting",
    "title": "Machine Learning Final Project",
    "section": "Boosting",
    "text": "Boosting\n\nimport random\nrandom.seed(1)\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n#While it is unnecessary to scale data for boosting, I include it so we can use it to remove outliers in the next pipeline. Scaling the data should not affect model performance.\nxgb_pipe = Pipeline(steps=[('scale', StandardScaler()), ('xgb', xgb.XGBClassifier(objective='multi:softprob'))]) #multi::softporb is used for multiclass outcome variables\n\nxgb_pipe_no_out = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('xgb', xgb.XGBClassifier(objective= 'multi:softprob'))]) \n\nxgb_pipe_bin = Pipeline(steps=[('scale', StandardScaler()), ('xgb', xgb.XGBClassifier(objective='binary:logistic'))]) #binary:logisitic is used for binary outcome variables\n\nxgb_pipe_no_out_bin = Pipeline(steps=[('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('xgb', xgb.XGBClassifier(objective= 'binary:logistic'))]) \n\n#setting parameters to grid search\nparams = {\n  \"xgb__n_estimators\": [50, 100, 300, 500, 1000], #while more trees could increase performance slightly for some models, this is too time consuming to run\n  \"xgb__learning_rate\": [0.01, 0.1, 0.5, 1],\n  \"xgb__max_depth\": [1,4,6,8] #max depth of a tree... I tried to tune to higher depths however this was very time consuming to run and didn't provide much in return/ could possibly lead to overfitting\n}#setting parameters to grid search\n\n#fixing class outcome labels (0,1,2, work with xgboost as opposed to 1,2,3)\nle = LabelEncoder()\ntrain_yb = le.fit_transform(train_y)\ntrain_yb_fac = le.fit_transform(train_y_fac)\n\n#fixing class of binary outcome to work with xgboost\ntrain_yb_bin = le.fit_transform(train_y_bin)\n\n#Implementing grid search with cross validation\nGS_xgb= GridSearchCV(xgb_pipe, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\nGS_xgb_no_out= GridSearchCV(xgb_pipe_no_out, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\n\nGS_xgb_bin= GridSearchCV(xgb_pipe_bin, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\nGS_xgb_no_out_bin= GridSearchCV(xgb_pipe_no_out_bin, param_grid=params, scoring = \"f1_macro\", cv=5, verbose=0)\n\n\n\nCV_F1_function(GS_object=GS_xgb, outcome=train_yb) #3 Classes\n\n{'xgb__learning_rate': 0.5, 'xgb__max_depth': 8, 'xgb__n_estimators': 50}\nBest Macro F1:  0.908639338113707\n[0.97113752 0.84367246 0.9124424 ]\n\nCV_F1_function(GS_object=GS_xgb_no_out, outcome=train_yb) #Outliers Removed\n\n{'xgb__learning_rate': 1, 'xgb__max_depth': 8, 'xgb__n_estimators': 300}\nBest Macro F1:  0.8891457194425483\n[0.97164621 0.81518987 0.88073394]\n\nCV_F1_function(GS_object=GS_xgb_bin, outcome=train_yb_bin) #Binary Outcome\n\n{'xgb__learning_rate': 0.1, 'xgb__max_depth': 8, 'xgb__n_estimators': 300}\nBest Macro F1:  0.9328886201134079\n[0.97193878 0.89423077]\n\nCV_F1_function(GS_object=GS_xgb_no_out_bin, outcome=train_yb_bin) #Outliers Removed\n\n{'xgb__learning_rate': 0.1, 'xgb__max_depth': 6, 'xgb__n_estimators': 1000}\nBest Macro F1:  0.9353604956079307\n[0.97274276 0.89808917]\n\n\nThese are the best performing models so far. The model with the binary outcome class (with outliers removed) performs fairly well on the suspect/pathological outcome class (macro f1= 0.935, f1 for “normal”=0.972, and f1 for “suspect/pathological”=0.898)."
  },
  {
    "objectID": "posts/Post 3/index.html#support-vector-machines-svm",
    "href": "posts/Post 3/index.html#support-vector-machines-svm",
    "title": "Machine Learning Final Project",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\n\nfrom sklearn import svm\n\nparams= {\n  'svm__C': [1, 10, 100, 200], #misclassification error term\n  'svm__gamma': [1, 0.1, 0.01, 0.001], #distance of points to decision boundary being considered\n  'svm__kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n}\n\nsvm_cl_pipe = Pipeline([('scale', StandardScaler()), ('svm', svm.SVC())])\n\nsvm_cl_pipe_no_out = Pipeline([('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('svm', svm.SVC())])\n\nGS_svm= GridSearchCV(svm_cl_pipe, param_grid=params, scoring = \"f1_macro\", cv=5)\nGS_svm_no_out= GridSearchCV(svm_cl_pipe_no_out, param_grid=params, scoring = \"f1_macro\", cv=5)\n\n\n\nCV_F1_function(GS_object=GS_svm, outcome=train_y) #Unordered 3 Classes\n\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.8468817212083832\n[0.95939086 0.73658537 0.85148515]\n\nCV_F1_function(GS_object=GS_svm_no_out, outcome=train_y) #Outliers Removed\n\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.8376867178470627\n[0.95379398 0.71921182 0.8436019 ]\n\nCV_F1_function(GS_object=GS_svm, outcome=train_y_bin) #Binary Outcome\n\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.904418810931511\n[0.96006797 0.8488746 ]\n\nCV_F1_function(GS_object=GS_svm_no_out, outcome=train_y_bin) #Outliers Removed\n\n{'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\nBest Macro F1:  0.8993948141236142\n[0.95737425 0.84126984]\n\n\nAgain the 3 class model has trouble detecting suspect cases, so in this case, I might prefer the binary outcome model (where suspect and pathological classes combined f1 score is 0.8489.) Boosting still performs better for both the three class and binary outcome variables."
  },
  {
    "objectID": "posts/Post 3/index.html#k-nearest-neighbors-knn",
    "href": "posts/Post 3/index.html#k-nearest-neighbors-knn",
    "title": "Machine Learning Final Project",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\nFinally I tested K nearest neighbors to see if a high variance model (low bias) may perform better than boosting. This is not the case.\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nparams = {'knn__n_neighbors': [1, 2, 3, 5, 10]}\n\nknn_pipe = Pipeline([('scale', StandardScaler()), ('knn', KNeighborsClassifier())])\n\nknn_pipe_no_out = Pipeline([('scale', StandardScaler()), ('elementwise_function', ElementWiseFunctionTransformer(replace_greater_than_abs_three)), ('knn', KNeighborsClassifier())])\n\nGS_knn= GridSearchCV(knn_pipe, param_grid=params, scoring = \"f1_macro\", cv=5)\nGS_knn_no_out= GridSearchCV(knn_pipe_no_out, param_grid=params, scoring = \"f1_macro\", cv=5)\n\n\n\nCV_F1_function(GS_object=GS_knn, outcome=train_y) #3 Classes\n\n{'knn__n_neighbors': 1}\nBest Macro F1:  0.8164438106575634\n[0.94661017 0.65835411 0.84651163]\n\nCV_F1_function(GS_object=GS_knn_no_out, outcome=train_y) #Outliers Removed\n\n{'knn__n_neighbors': 3}\nBest Macro F1:  0.7970359617201562\n[0.94745621 0.66666667 0.77832512]\n\nCV_F1_function(GS_object=GS_knn, outcome=train_y_bin) #Binary Outcome\n\n{'knn__n_neighbors': 3}\nBest Macro F1:  0.8756797607140687\n[0.9510665 0.8      ]\n\nCV_F1_function(GS_object=GS_knn_no_out, outcome=train_y_bin) #Outliers Removed\n\n{'knn__n_neighbors': 3}\nBest Macro F1:  0.8781885851854287\n[0.95130143 0.8047138 ]"
  },
  {
    "objectID": "posts/Post 3/index.html#final-model-evaluation",
    "href": "posts/Post 3/index.html#final-model-evaluation",
    "title": "Machine Learning Final Project",
    "section": "Final Model Evaluation",
    "text": "Final Model Evaluation\n\nTest set pre-processing\n\n#Test set pre-processing\n#Setting subsets for x and y variables\n\ntest_x=test[['baseline.value',\n'accelerations',\n'fetal_movement',\n'uterine_contractions',\n'light_decelerations',\n'severe_decelerations',\n'prolongued_decelerations',\n'abnormal_short_term_variability',\n'mean_value_of_short_term_variability',\n'percentage_of_time_with_abnormal_long_term_variability',\n'mean_value_of_long_term_variability',\n'histogram_width',\n'histogram_min',\n'histogram_max',\n'histogram_number_of_peaks',\n'histogram_number_of_zeroes',\n'histogram_mode',\n'histogram_mean',\n'histogram_median',\n'histogram_variance',\n'histogram_tendency_negative',\n'histogram_tendency_zero',\n'histogram_tendency_positive']]\n\n#Outcome Class for 3 Classes\ntest_y=test[['fetal_health']]\n\n#Outcome Class for Binary\ntest_y_bin=test[['fetal_health_bin']]\n\n#convert data type from matrix to numpy npdarray\ntest_y=test_y.values.ravel()\ntest_y_bin=test_y_bin.values.ravel()\n\n#Standardize Test Data Based on column means and standard deviations of training set\ntest_x = (test_x - train_x.mean()) / train_x.std()\ntest_x_no_out=replace_greater_than_abs_three(test_x) #create test df with outliers imputed with the mean (0)\n\n#For simplicity, all variables (including dummy variables) have been standardized (the pipeline did this as well). While unnecessary, this should not affect results. \n\n#Standardize full training data\ntrain_x_st = (train_x - train_x.mean()) / train_x.std()\n\ntrain_x_st_no_out=replace_greater_than_abs_three(train_x_st) #create training df with outliers imputed with the mean\n\n\n\nLogistic Regression\n\nrandom.seed(3)\n#3 Classes, Outliers Kept: l1 penalty with C=1\nlog_final_mod=LogisticRegression(solver='saga', tol=0.006, C=1.0, penalty='l1') #fit model with optimized hyperparameters\nlog_final_mod.fit(train_x_st, train_y)\n\nLogisticRegression(penalty='l1', solver='saga', tol=0.006)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='l1', solver='saga', tol=0.006)\n\ny_pred=log_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.7603825352102777\n\nf1_by_class = f1_score(test_y, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome, Outliers Kept: l1 penalty and C=500\n\n[0.9402229  0.58682635 0.75409836]\n\nlog_final_mod_bin=LogisticRegression(solver='saga', tol=0.006, C=500, penalty='l1') #fit model with optimized hyperparameters\nlog_final_mod.fit(train_x_st, train_y_bin)\n\nLogisticRegression(penalty='l1', solver='saga', tol=0.006)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='l1', solver='saga', tol=0.006)\n\ny_pred=log_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.8712133867914023\n\nf1_by_class = f1_score(test_y_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n[0.94105691 0.80136986]\n\n\n\n\nBoosting\n\nrandom.seed(3)\n#fixing class outcome labels (0,1,2, work with xgboost as opposed to 1,2,3)\nle = LabelEncoder()\ntest_yb = le.fit_transform(test_y)\n\n#fixing class of binary outcome to work with xgboost\ntest_yb_bin = le.fit_transform(test_y_bin)\n\n#3 Class, Outliers Kept: learning_rate= 0.5, max_depth= 8, n_estimators= 50\nboosting_final_mod= xgb.XGBClassifier(objective='multi:softprob', learning_rate= 0.5, max_depth= 8, n_estimators= 50)\nboosting_final_mod.fit(train_x_st, train_yb)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=8, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              objective='multi:softprob', predictor=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=8, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              objective='multi:softprob', predictor=None, ...)\n\ny_pred=boosting_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_yb, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.9241664606695282\n\nf1_by_class = f1_score(test_yb, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome, Outliers Imputed: learning_rate=0.1, max_depth= 6, n_estimators= 1000\n\n[0.97341513 0.8452381  0.95384615]\n\nboosting_final_mod_bin= xgb.XGBClassifier(objective='binary:logistic', learning_rate= 0.1, max_depth= 6, n_estimators= 1000)\nboosting_final_mod_bin.fit(train_x_st_no_out, train_yb_bin)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=1000, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n\ny_pred=boosting_final_mod_bin.predict(test_x_no_out) #make predictions on test set\n\nf1_1 = f1_score(test_yb_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.9474616049738545\n\nf1_by_class = f1_score(test_yb_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n[0.97546012 0.91946309]\n\n\n\n\nSupport Vector Machines\n\nrandom.seed(3)\n#3 Class Outcome, Outliers Kept: C=10, gamma= 0.1, kernel= 'rbf'\nsvm_final_mod=svm.SVC(C=10, gamma= 0.1, kernel= 'rbf')\n\nsvm_final_mod.fit(train_x_st, train_y)\n\nSVC(C=10, gamma=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=10, gamma=0.1)\n\ny_pred=svm_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.8413941228023978\n\nf1_by_class = f1_score(test_y, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome Outliers Kept: C=10, gamma= 0.1, kernel= 'rbf'\n\n[0.95634518 0.78212291 0.78571429]\n\nsvm_final_mod_bin=svm.SVC(C=10, gamma= 0.1, kernel= 'rbf')\n\nsvm_final_mod_bin.fit(train_x_st, train_y_bin)\n\nSVC(C=10, gamma=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=10, gamma=0.1)\n\ny_pred=svm_final_mod_bin.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.9124360082897572\n\nf1_by_class = f1_score(test_y_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n[0.9591002  0.86577181]\n\n\n\n\nK-Nearest Neighbors\n\nrandom.seed(3)\n#3 Class, Outliers Kept: n-neighbors= 1\nknn_final_mod=KNeighborsClassifier(n_neighbors=1)\nknn_final_mod.fit(train_x_st, train_y)\n\nKNeighborsClassifier(n_neighbors=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=1)\n\ny_pred=knn_final_mod.predict(test_x) #make predictions on test set\n\nf1_1 = f1_score(test_y, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.7923969108297486\n\nf1_by_class = f1_score(test_y, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n#Binary Outcome, Outliers Imputed: n-neighbors=3\n\n[0.94512195 0.65895954 0.77310924]\n\nknn_final_mod_bin=KNeighborsClassifier(n_neighbors=3)\nknn_final_mod_bin.fit(train_x_st_no_out, train_y_bin)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)\n\ny_pred=knn_final_mod_bin.predict(test_x_no_out) #make predictions on test set\n\nf1_1 = f1_score(test_y_bin, y_pred, average='macro') #calculate macro f1\nprint(f1_1)\n\n0.8512755510876059\n\nf1_by_class = f1_score(test_y_bin, y_pred, average=None) #calculate by-class f1 scores\nprint(f1_by_class)\n\n[0.93612774 0.76642336]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emma Rasmussen",
    "section": "",
    "text": "Dec 10, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThe Effect of Seasonality on Mass Shootings: An Exploratory Data Analysis\n\n\n\n\n\n\nSep 3, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMachine Learning to Interpret Cardiotocograms (CTGs)\n\n\n\n\n\n\nMay 22, 2022\n\n\nEmma Rasmussen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "To work towards this passion, I am a current student in the Data Analytics and Computation Social Sciences (DACSS) program at the University of Massachusetts Amherst. I have an undergraduate degree in public health and anthropology/sociology. After completing my undergraduate degree I worked as a research assistant with the Framingham Heart Study which involved manually extracting data from individual health records. This led to my interest in the analysis portion of health data and making the process more efficient. I cannot wait to combine my interests to do meaningful work in public health data analysis in the future.\nThis blog has examples of my work from courses I have taken in my time at DACSS. Please feel free to browse my work, and do not hesitate to contact me if you have any questions.\nEmma\nerasmussen@umass.edu"
  }
]